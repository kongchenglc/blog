<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Mike&#039;s Blog</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Mike&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Mike&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Mike&#039;s Blog"><meta property="og:url" content="https://kongchenglc.github.io/blog"><meta property="og:site_name" content="Mike&#039;s Blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://kongchenglc.github.io/blog/img/og_image.png"><meta property="article:author" content="Cheng (Mike)"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://kongchenglc.github.io/blog/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kongchenglc.github.io/blog"},"headline":"Mike's Blog","image":["https://kongchenglc.github.io/blog/img/og_image.png"],"author":{"@type":"Person","name":"Cheng (Mike)"},"publisher":{"@type":"Organization","name":"Mike's Blog","logo":{"@type":"ImageObject","url":{"text":"Mike's Blog"}}},"description":""}</script><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/">Mike&#039;s Blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/blog/">Home</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-07-24T21:50:04.000Z" title="7/24/2025, 9:50:04 PM">2025-07-24</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Roadmap/">Roadmap</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/07/24/FrontEnd-Atlas/">FrontEnd Atlas</a></p><div class="content"><h1 id="Fundamentals"><a href="#Fundamentals" class="headerlink" title="Fundamentals"></a>Fundamentals</h1><h2 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h2><h3 id="SEO-Search-Engine-Optimization"><a href="#SEO-Search-Engine-Optimization" class="headerlink" title="SEO (Search Engine Optimization)"></a>SEO (Search Engine Optimization)</h3><ul>
<li>Use semantic HTML elements (<code>&lt;article&gt;, &lt;section&gt;, &lt;nav&gt;, &lt;header&gt;, &lt;h1&gt;&lt;h2&gt;</code> etc.)</li>
<li>Set meaningful <code>&lt;title&gt;</code> and <code>&lt;meta name=&quot;description&quot; content=&quot;xxx&quot;&gt;</code> for each page.</li>
<li>Use of <code>&lt;alt&gt;</code> Attributes on Images</li>
<li>Use <code>&lt;meta name=&quot;robots&quot;&gt;</code> or <code>robots.txt</code> to control indexing.</li>
</ul>
<h3 id="Render-Process"><a href="#Render-Process" class="headerlink" title="Render Process"></a>Render Process</h3><p>DOM + CSSOM &#x3D;&gt; Render Tree<br>Layout(Size, Position) &#x3D;&gt; Paint(Color, Background) &#x3D;&gt; Composite<br>Recalculate Style &#x3D;&gt; Reflow &#x3D;&gt; Repaint &#x3D;&gt; Compositing</p></div><a class="article-more button is-small is-size-7" href="/blog/2025/07/24/FrontEnd-Atlas/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-05-31T03:18:23.000Z" title="5/31/2025, 3:18:23 AM">2025-05-31</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Roadmap/">Roadmap</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/05/31/algorithm-methodology/">Algorithms</a></p><div class="content"><h1 id="Problem-Solving-Steps"><a href="#Problem-Solving-Steps" class="headerlink" title="Problem Solving Steps"></a>Problem Solving Steps</h1><ol>
<li><strong>Communication</strong>: The goal is to solve real-world problems collaboratively with the interviewer.</li>
<li><strong>Clarify the Objective</strong>: Confirming what is asked is very important. For example, if the problem requires returning values instead of indices, then storing index information is meaningless.<ul>
<li>Identify boundary cases.</li>
<li>Ask valuable questions.</li>
<li>Don’t rush to code; first confirm you are solving the correct problem.</li>
</ul>
</li>
<li><strong>Formulate the Problem</strong>: To solve algorithm questions, you need to build a formula that describes how to get the result. Then look at all known conditions (such as whether the data is sorted, or any useful data characteristics). This helps decide how the formula converges and whether to use loops or recursion.</li>
<li><strong>Explain Your Approach</strong>: Before coding, explain your thought process to the interviewer and get confirmation.</li>
<li><strong>Code</strong>:<ul>
<li>Use a consistent coding style. Use meaningful variable names, and comments.</li>
<li>Avoid premature optimization; focus on correctness first.</li>
<li>If you are stuck, try to simplify the problem or break it down into smaller parts.</li>
</ul>
</li>
<li><strong>Test Your Code</strong>: After coding, run several test cases including edge cases. Run the code manually step by step.</li>
<li><strong>Optimize</strong>: Improve time and space complexity if possible. Dont forget that.</li>
</ol></div><a class="article-more button is-small is-size-7" href="/blog/2025/05/31/algorithm-methodology/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-03-11T21:21:59.000Z" title="3/11/2025, 9:21:59 PM">2025-03-11</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/03/11/Machine-Learning-10/">About Machine Learning ( Part 10: Reinforcement Learning )</a></p><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><strong>Reinforcement Learning (RL)</strong> is a fascinating branch of machine learning where an agent learns to interact with an environment to maximize long-term cumulative rewards. Unlike supervised learning, RL relies on feedback through interaction instead of labeled data.</p>
<p>The core of RL is built upon <strong>Markov Decision Processes (MDPs)</strong>, which provide a mathematical framework for modeling decision-making under uncertainty.</p>
<p>This blog post explores the key components of RL, including value functions, Q-functions, the Bellman equation, Actor-Critic architectures, PPO, and commonly used tools in real-world RL implementations.</p></div><a class="article-more button is-small is-size-7" href="/blog/2025/03/11/Machine-Learning-10/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-03-11T21:05:34.000Z" title="3/11/2025, 9:05:34 PM">2025-03-11</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/03/11/Transformer-3/">Transformer ( Part 3: Transformer Architecture )</a></p><div class="content"><h1 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder &amp; Decoder"></a>Encoder &amp; Decoder</h1><p>The Transformer consists of two main parts: an <strong>encoder</strong> and a <strong>decoder</strong>. They are connected by Cross-Attention.</p>
<ul>
<li><strong>Encoder</strong>: Processes the input sequence using multiple layers of self-attention and feed-forward networks.</li>
<li><strong>Decoder</strong>: Takes the encoder’s output and generates the target sequence using self-attention and cross-attention mechanisms.</li>
</ul>
<p>The Transformer Architecture:</p>
<p><img src="/blog/./img/Transformer3-1.png" alt="https://arxiv.org/pdf/1706.03762"></p></div><a class="article-more button is-small is-size-7" href="/blog/2025/03/11/Transformer-3/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-03-01T14:51:44.000Z" title="3/1/2025, 2:51:44 PM">2025-03-01</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/03/01/Transformer-2/">Transformer ( Part 2: Multi-Head Attention )</a></p><div class="content"><p>Before the Transformer, sequence models like RNNs and LSTMs suffered from <strong>long-term dependency issues</strong> and <strong>low parallelization efficiency</strong>. Self-Attention was introduced as an alternative, allowing for <strong>parallel computation</strong> and capturing <strong>long-range dependencies</strong>.</p>
<p>However, a single-head Self-Attention mechanism has a limitation:<br><strong>It can only focus on one type of relationship or pattern in the data.</strong>  </p>
<p>Multi-Head Attention overcomes this by <strong>using multiple attention heads</strong> that capture different aspects of the input, improving the model’s expressiveness.</p></div><a class="article-more button is-small is-size-7" href="/blog/2025/03/01/Transformer-2/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-12T17:00:56.000Z" title="2/12/2025, 5:00:56 PM">2025-02-12</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/12/Machine-Learning-9/">About Machine Learning ( Part 9: Recurrent Neural Network )</a></p><div class="content"><p>Recurrent Neural Networks (RNNs) are a class of neural networks designed for <strong>sequential data</strong>, making them highly effective for tasks like <strong>natural language processing (NLP), time series prediction, and speech recognition</strong>. Unlike traditional feedforward networks, RNNs maintain a hidden state that captures <strong>temporal dependencies</strong>.</p>
<h2 id="How-RNNs-Work"><a href="#How-RNNs-Work" class="headerlink" title="How RNNs Work"></a>How RNNs Work</h2><p>A traditional <strong>feedforward neural network</strong> processes inputs independently. However, for sequential tasks, the <strong>order</strong> of the data is crucial. <strong>RNNs address this by maintaining a memory of previous inputs through hidden states.</strong></p></div><a class="article-more button is-small is-size-7" href="/blog/2025/02/12/Machine-Learning-9/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-12T16:33:49.000Z" title="2/12/2025, 4:33:49 PM">2025-02-12</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/12/Machine-Learning-8/">About Machine Learning ( Part 8: Convolution Neural Networks )</a></p><div class="content"><p>Convolutional Neural Networks (CNNs) have revolutionized the field of computer vision, enabling significant advancements in image recognition, object detection, and segmentation tasks. This blog will explore the key concepts behind CNNs and their working principles.</p>
<h2 id="What-is-a-CNN"><a href="#What-is-a-CNN" class="headerlink" title="What is a CNN?"></a>What is a CNN?</h2><p>A Convolutional Neural Network (CNN) is a type of deep learning model specifically designed for processing structured grid data, such as images. Unlike traditional fully connected neural networks, CNNs leverage <strong>convolutional layers</strong> to capture spatial hierarchies in the data.</p></div><a class="article-more button is-small is-size-7" href="/blog/2025/02/12/Machine-Learning-8/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-09T21:03:49.000Z" title="2/9/2025, 9:03:49 PM">2025-02-09</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/09/Transformer-1/">Transformer ( Part 1: Word Embedding )</a></p><div class="content"><p>Word Embedding is one of the most fundamental techniques in Natural Language Processing (NLP). It represents words as continuous vectors in a high-dimensional space, capturing semantic relationships between them.  </p>
<h2 id="Why-Do-We-Need-Word-Embeddings"><a href="#Why-Do-We-Need-Word-Embeddings" class="headerlink" title="Why Do We Need Word Embeddings?"></a>Why Do We Need Word Embeddings?</h2><p>Before word embeddings, one common method to represent words was <strong>One-Hot Encoding</strong>. In this approach, each word is represented as a high-dimensional sparse vector.  </p>
<p>For example, if our vocabulary has 10,000 words, we encode each word as:<br>$$<br>\text{dog} &#x3D; [0, 1, 0, 0, \dots, 0]<br>$$<br>However, this method has significant drawbacks:  </p>
<ol>
<li><strong>High dimensionality</strong> – A large vocabulary results in enormous vectors.  </li>
<li><strong>No semantic similarity</strong> – “dog” and “cat” are conceptually related, but their one-hot vectors are completely different.</li>
</ol>
<p>Word embeddings solve these issues by <strong>learning low-dimensional, dense representations</strong> that encode semantic relationships between words.  </p>
<p><img src="/blog/./img/Transformer1-1.png" alt="https://corpling.hypotheses.org/495"></p></div><a class="article-more button is-small is-size-7" href="/blog/2025/02/09/Transformer-1/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-06T21:43:39.000Z" title="2/6/2025, 9:43:39 PM">2025-02-06</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/06/Machine-Learning-7/">About Machine Learning ( Part 7: Artificial Neural Network )</a></p><div class="content"><h1 id="Bayes’-theorem"><a href="#Bayes’-theorem" class="headerlink" title="Bayes’ theorem"></a>Bayes’ theorem</h1><p>$$<br>P(y|X) &#x3D; \frac{P(X|y) P(y)}{P(X)}<br>$$</p>
<p>where:</p>
<ul>
<li>$P(y|X)$: Posterior probability of class $y$ given input $X$.</li>
<li>$P(X|y)$: Likelihood of seeing $X$ if the class is $y$.</li>
<li>$P(y)$: Prior probability of class $y$.</li>
<li>$P(X)$: Total probability of $X$ (normalization factor).</li>
</ul>
<h2 id="Bayes-Network-Bayesian-Network-BN"><a href="#Bayes-Network-Bayesian-Network-BN" class="headerlink" title="Bayes Network (Bayesian Network, BN)"></a>Bayes Network (Bayesian Network, BN)</h2><p>A <strong>Bayesian network (BN)</strong> is a <strong>graphical model</strong> representing probabilistic dependencies between variables. It consists of:</p>
<ul>
<li><strong>Nodes:</strong> Represent variables (e.g., symptoms, diseases).</li>
<li><strong>Edges:</strong> Represent conditional dependencies.</li>
</ul>
<p><img src="/blog/./img/ML7-1.png"></p></div><a class="article-more button is-small is-size-7" href="/blog/2025/02/06/Machine-Learning-7/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-02T18:20:57.000Z" title="2/2/2025, 6:20:57 PM">2025-02-02</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/02/Machine-Learning-6/">About Machine Learning ( Part 6: KNN vs. K-means )</a></p><div class="content"><p>In machine learning, <strong>K-Nearest Neighbors (KNN)</strong> and <strong>K-means Clustering</strong> are two commonly used algorithms. Despite their similar names, they serve <strong>different purposes</strong> and have <strong>distinct working principles</strong>.  </p>
<h1 id="KNN-K-Nearest-Neighbors"><a href="#KNN-K-Nearest-Neighbors" class="headerlink" title="KNN (K-Nearest Neighbors)"></a>KNN (K-Nearest Neighbors)</h1><p>KNN is a <strong>supervised learning</strong> algorithm used for <strong>classification</strong> and <strong>regression</strong> tasks.  </p>
<p>The core idea of KNN is:</p>
<blockquote>
<p>Given a new data point, find the <strong>K</strong> most similar instances in the training dataset (neighbors) and use them to predict the output.</p>
</blockquote>
<p>KNN is a <strong>lazy learning</strong> algorithm, meaning it does not require a training phase. Instead, it directly classifies or predicts based on stored data.</p></div><a class="article-more button is-small is-size-7" href="/blog/2025/02/02/Machine-Learning-6/#more">Read more</a></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/blog/page/0/">Previous</a></div><div class="pagination-next"><a href="/blog/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/blog/">1</a></li><li><a class="pagination-link" href="/blog/page/2/">2</a></li><li><a class="pagination-link" href="/blog/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/blog/img/profile-pic.jpg" alt="Cheng (Mike)"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Cheng (Mike)</p><p class="is-size-6 is-block">Software Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Canada</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/blog/archives"><p class="title">24</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/blog/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">40</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/kongchenglc" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Linkedin" href="https://www.linkedin.com/in/kongchenglc/"><i class="fab fa-linkedin"></i></a></div></div></div><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/">Mike&#039;s Blog</a><p class="is-size-7"><span>&copy; 2025 Cheng (Mike)</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2024</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/blog/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><!--!--><script data-pjax src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><!-- hexo injector body_end start -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    var tocElement = document.querySelector('#toc');
    if (tocElement) {
      tocElement.classList.add('is-sticky');
    }
  });
</script>
  <!-- hexo injector body_end end --></body></html>