<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Category: Learning Notes - Lich&#039;s Blog</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Lich&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lich&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Lich&#039;s Blog"><meta property="og:url" content="https://kongchenglc.github.io/blog"><meta property="og:site_name" content="Lich&#039;s Blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://kongchenglc.github.io/blog/img/og_image.png"><meta property="article:author" content="Lich"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://kongchenglc.github.io/blog/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kongchenglc.github.io/blog"},"headline":"Lich's Blog","image":["https://kongchenglc.github.io/blog/img/og_image.png"],"author":{"@type":"Person","name":"Lich"},"publisher":{"@type":"Organization","name":"Lich's Blog","logo":{"@type":"ImageObject","url":{"text":"Lich's Blog"}}},"description":""}</script><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/">Lich&#039;s Blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">Home</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/blog/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">Learning Notes</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-13T23:16:44.000Z" title="2/13/2025, 11:16:44 PM">2025-02-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-02-16T02:45:33.363Z" title="2/16/2025, 2:45:33 AM">2025-02-16</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/13/Transformer-2/">Transformer ( Part 2: Multi-Head Attention )</a></p><div class="content"></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-12T17:00:56.000Z" title="2/12/2025, 5:00:56 PM">2025-02-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-02-16T02:45:33.363Z" title="2/16/2025, 2:45:33 AM">2025-02-16</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/12/Machine-Learning-9/">About Machine Learning ( Part 9: Recurrent Neural Network )</a></p><div class="content"><p>Recurrent Neural Networks (RNNs) are a class of neural networks designed for <strong>sequential data</strong>, making them highly effective for tasks like <strong>natural language processing (NLP), time series prediction, and speech recognition</strong>. Unlike traditional feedforward networks, RNNs maintain a hidden state that captures <strong>temporal dependencies</strong>.</p>
<h2 id="How-RNNs-Work"><a href="#How-RNNs-Work" class="headerlink" title="How RNNs Work"></a>How RNNs Work</h2><p>A traditional <strong>feedforward neural network</strong> processes inputs independently. However, for sequential tasks, the <strong>order</strong> of the data is crucial. <strong>RNNs address this by maintaining a memory of previous inputs through hidden states.</strong></p>
<h3 id="Mathematical-Representation"><a href="#Mathematical-Representation" class="headerlink" title="Mathematical Representation"></a>Mathematical Representation</h3><p>At each time step $t$, an RNN takes the <strong>current input</strong> $x_t$ and the <strong>previous hidden state</strong> $h_{t-1}$ to compute the <strong>new hidden state</strong> $h_t$:</p>
<p>$$<br>h_t &#x3D; f(W_h h_{t-1} + W_x x_t + b_h)<br>$$</p>
<p>where:</p>
<ul>
<li>$W_h$ and $W_x$ are weight matrices,</li>
<li>$b_h$ is the bias term,</li>
<li>$f$ is usually a non-linear activation function like $ \tanh $,</li>
<li>$h_t$ represents the <strong>memory</strong> of past computations.</li>
</ul>
<p>Finally, the <strong>output</strong> $o_t$ is computed as:</p>
<p>$$<br>o_t &#x3D; g(W_y h_t + b_y)<br>$$</p>
<p>where $g$ is often a softmax function for classification tasks.</p>
<p><img src="/blog/./img/ML9-1.png" alt="http://dprogrammer.org/rnn-lstm-gru"></p>
<h2 id="The-Vanishing-Gradient-Problem"><a href="#The-Vanishing-Gradient-Problem" class="headerlink" title="The Vanishing Gradient Problem"></a>The Vanishing Gradient Problem</h2><p>A major challenge in training RNNs is the <strong>vanishing gradient problem</strong>.<br>During backpropagation, gradients can shrink exponentially when passing through many time steps, making it difficult to <strong>learn long-range dependencies</strong>.</p>
<p>To address this issue, <strong>LSTMs (Long Short-Term Memory)</strong> and <strong>GRUs (Gated Recurrent Units)</strong> were introduced.</p>
<h2 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long Short-Term Memory (LSTM)"></a>Long Short-Term Memory (LSTM)</h2><p>LSTMs introduce <strong>gates</strong> to control how much past information should be retained or discarded. This helps <strong>preserve long-term dependencies</strong>.</p>
<p>Each LSTM unit has:</p>
<ol>
<li><strong>Forget Gate</strong>: Decides what information to discard<br>$$<br>f_t &#x3D; \sigma(W_f h_{t-1} + U_f x_t + b_f)<br>$$</li>
<li><strong>Input Gate</strong>: Determines new information to store<br>$$<br>i_t &#x3D; \sigma(W_i h_{t-1} + U_i x_t + b_i)<br>$$</li>
<li><strong>Cell State Update</strong>: Computes the candidate memory</li>
</ol>
<p>$$<br>\tilde{C_t} &#x3D; \tanh(W_c h_{t-1} + U_c x_t + b_c)<br>$$</p>
<p>Then updates the cell state:</p>
<p>$$<br>C_t &#x3D; f_t C_{t-1} + i_t \tilde{C}_t<br>$$</p>
<ol start="4">
<li><strong>Output Gate</strong>: Determines the final hidden state<br>$$<br>o_t &#x3D; \sigma(W_o h_{t-1} + U_o x_t + b_o)<br>$$<br>$$<br>h_t &#x3D; o_t \tanh(C_t)<br>$$</li>
</ol>
<p>LSTMs ensure that important past information <strong>remains accessible over long sequences</strong>.</p>
<p><img src="/blog/./img/ML9-2.png" alt="http://dprogrammer.org/rnn-lstm-gru"></p>
<h3 id="Cell-State-C-t"><a href="#Cell-State-C-t" class="headerlink" title="Cell State $C_t$"></a>Cell State $C_t$</h3><p>The <strong>cell state</strong> can be considered as the “memory” of the LSTM network. It carries information across time steps, and it’s responsible for helping the network preserve <strong>long-term dependencies</strong>. The cell state is essentially the backbone of an LSTM that allows it to remember information over long sequences.</p>
<h4 id="Update-Mechanism-of-Cell-State"><a href="#Update-Mechanism-of-Cell-State" class="headerlink" title="Update Mechanism of Cell State"></a>Update Mechanism of Cell State</h4><p>The cell state is updated through two important gates in the LSTM:</p>
<ol>
<li><strong>Forget Gate</strong> ($f_t$) - Decides what information should be discarded.</li>
<li><strong>Input Gate</strong> ($i_t$) - Determines what new information should be added to the memory.</li>
</ol>
<p>The cell state $C_t$ is updated as follows:</p>
<p>$$<br>C_t &#x3D; f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t<br>$$</p>
<p>Where:</p>
<ul>
<li>$C_{t-1}$ is the previous cell state.</li>
<li>$f_t$ is the forget gate’s output, which decides how much of the previous cell state should be remembered.</li>
<li>$i_t$ is the input gate’s output, which decides how much new information should be stored in the cell state.</li>
<li>$\tilde{C}_t$ is the candidate cell state, which represents the new potential memory.</li>
</ul>
<h3 id="Hidden-State-h-t"><a href="#Hidden-State-h-t" class="headerlink" title="Hidden State $h_t$"></a>Hidden State $h_t$</h3><p>The <strong>hidden state</strong> is the network’s <strong>short-term memory</strong>. It represents the <strong>output</strong> of the LSTM at each time step and carries information relevant to the current time step’s computation. The hidden state is the value that is passed to the next time step and is typically used to generate predictions or outputs.</p>
<h4 id="Update-Mechanism-of-Hidden-State"><a href="#Update-Mechanism-of-Hidden-State" class="headerlink" title="Update Mechanism of Hidden State"></a>Update Mechanism of Hidden State</h4><p>The hidden state $h_t$ is derived from the current cell state $C_t$ using the <strong>output gate</strong> ($o_t$). The output gate decides how much of the current cell state should be exposed as the hidden state:</p>
<p>$$<br>h_t &#x3D; o_t \cdot \tanh(C_t)<br>$$</p>
<p>Where:</p>
<ul>
<li>$o_t$ is the output gate, which determines how much of the cell state should be visible in the hidden state.</li>
<li>$\tanh(C_t)$ is the cell state passed through the $\tanh$ activation function.</li>
</ul>
<h3 id="Differences-Between-C-t-and-h-t"><a href="#Differences-Between-C-t-and-h-t" class="headerlink" title="Differences Between $C_t$ and $h_t$"></a>Differences Between $C_t$ and $h_t$</h3><p>While both <strong>cell state</strong> ($C_t$) and <strong>hidden state</strong> ($h_t$) are crucial in LSTM networks, they serve distinct roles:</p>
<ul>
<li><p><strong>$C_t$ (Cell State)</strong>: </p>
<ul>
<li>Acts as the <strong>long-term memory</strong> of the network.</li>
<li>It is designed to carry information over long time periods, ensuring the network remembers relevant data from earlier time steps.</li>
<li>The cell state is passed through the time steps with minimal changes unless explicitly modified by the forget and input gates.</li>
</ul>
</li>
<li><p><strong>$h_t$ (Hidden State)</strong>:</p>
<ul>
<li>Acts as the <strong>short-term memory</strong>.</li>
<li>It contains information that is relevant to the current time step and is used to generate the output of the LSTM at each time step.</li>
<li>The hidden state is passed to the next time step as the updated memory, which is used for prediction or classification.</li>
</ul>
</li>
</ul>
<h2 id="Gated-Recurrent-Unit-GRU"><a href="#Gated-Recurrent-Unit-GRU" class="headerlink" title="Gated Recurrent Unit (GRU)"></a>Gated Recurrent Unit (GRU)</h2><p>GRUs are another variant of RNNs introduced to improve the learning of long-range dependencies. GRUs are <strong>simpler</strong> than LSTMs, as they use fewer gates, which can lead to faster training times while still addressing the vanishing gradient problem.</p>
<p><img src="/blog/./img/ML9-3.png" alt="http://dprogrammer.org/rnn-lstm-gru"></p>
<p>A GRU unit consists of two main gates:</p>
<ol>
<li><p><strong>Update Gate</strong>: This gate decides how much of the past information should be carried forward.<br>$$<br>z_t &#x3D; \sigma(W_z h_{t-1} + U_z x_t + b_z)<br>$$</p>
<p> $z_t &#x3D; 0$: The model keeps the previous hidden state $h_{t-1}$ and does not update with new information.<br> $z_t &#x3D; 1$: The model fully updates the hidden state with the new candidate hidden state $\tilde{h}_t$.</p>
</li>
<li><p><strong>Reset Gate</strong>: This gate determines how much of the past hidden state should be forgotten.<br>$$<br>r_t &#x3D; \sigma(W_r h_{t-1} + U_r x_t + b_r)<br>$$</p>
<p> $r_t &#x3D; 0$: Forget the previous hidden state.<br> $r_t &#x3D; 1$: Use the previous hidden state fully.</p>
</li>
</ol>
<p>The candidate hidden state $\tilde{h}_t$ is computed as:</p>
<p>$$<br>\tilde{h_t} &#x3D; \tanh(W_h (r_t \cdot h_{t-1}) + U_h x_t + b_h)<br>$$</p>
<p>Finally, the hidden state $h_t$ is updated as a combination of the previous hidden state and the candidate hidden state:</p>
<p>$$<br>h_t &#x3D; (1 - z_t) \cdot h_{t-1} + z_t \cdot \tilde{h}_t<br>$$</p>
<p>The <strong>update gate</strong> controls how much of the previous hidden state is kept, while the <strong>reset gate</strong> determines how much of the past information is discarded.</p>
<p>GRUs are computationally more efficient than LSTMs because they have fewer parameters to train, yet often perform comparably in many tasks.</p>
<h2 id="LSTM-vs-GRU-Key-Differences"><a href="#LSTM-vs-GRU-Key-Differences" class="headerlink" title="LSTM vs. GRU: Key Differences"></a>LSTM vs. GRU: Key Differences</h2><p>While both LSTMs and GRUs address the vanishing gradient problem, their architecture and gate structure differ:</p>
<ol>
<li><p><strong>Number of Gates</strong>:</p>
<ul>
<li><strong>LSTM</strong> has <strong>three gates</strong>: Forget gate, Input gate, and Output gate.</li>
<li><strong>GRU</strong> has only <strong>two gates</strong>: Update gate and Reset gate.</li>
</ul>
</li>
<li><p><strong>Complexity</strong>:</p>
<ul>
<li><strong>LSTM</strong> is generally <strong>more complex</strong> and has more parameters due to the extra gates and the cell state.</li>
<li><strong>GRU</strong> is <strong>simpler</strong> with fewer parameters, making it computationally more efficient.</li>
</ul>
</li>
<li><p><strong>Performance</strong>:</p>
<ul>
<li><strong>LSTM</strong> might perform better on some tasks due to its ability to separately maintain the cell state and hidden state, but it may require more data and computational resources.</li>
<li><strong>GRU</strong>, being simpler, can often match or even outperform LSTM on smaller datasets or simpler tasks.</li>
</ul>
</li>
<li><p><strong>Use Cases</strong>:</p>
<ul>
<li><strong>LSTM</strong> is typically used when the task requires more complex learning of long-term dependencies.</li>
<li><strong>GRU</strong> is useful for faster training and can be an effective choice for tasks where slightly fewer gates might still work well.</li>
</ul>
</li>
</ol>
<p>In many practical scenarios, both LSTM and GRU can give similar results. The choice between the two often comes down to the specific task, available computational resources, and performance requirements.</p>
<h2 id="Applications-of-RNNs"><a href="#Applications-of-RNNs" class="headerlink" title="Applications of RNNs"></a>Applications of RNNs</h2><p>RNNs and their variants (LSTM, GRU) are widely used in:</p>
<p><strong>Natural Language Processing (NLP)</strong></p>
<ul>
<li>Sentiment analysis</li>
<li>Machine translation (Google Translate)</li>
<li>Chatbots &amp; conversational AI</li>
</ul>
<p><strong>Time Series Forecasting</strong></p>
<ul>
<li>Stock price prediction</li>
<li>Weather forecasting</li>
</ul>
<p><strong>Speech Recognition</strong></p>
<ul>
<li>Voice assistants like Siri &amp; Google Assistant</li>
</ul>
<p><strong>Music Generation</strong></p>
<ul>
<li>AI-generated compositions</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-12T16:33:49.000Z" title="2/12/2025, 4:33:49 PM">2025-02-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-02-16T02:45:33.363Z" title="2/16/2025, 2:45:33 AM">2025-02-16</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/12/Machine-Learning-8/">About Machine Learning ( Part 8: Convolution Neural Networks )</a></p><div class="content"><p>Convolutional Neural Networks (CNNs) have revolutionized the field of computer vision, enabling significant advancements in image recognition, object detection, and segmentation tasks. This blog will explore the key concepts behind CNNs and their working principles.</p>
<h2 id="What-is-a-CNN"><a href="#What-is-a-CNN" class="headerlink" title="What is a CNN?"></a>What is a CNN?</h2><p>A Convolutional Neural Network (CNN) is a type of deep learning model specifically designed for processing structured grid data, such as images. Unlike traditional fully connected neural networks, CNNs leverage <strong>convolutional layers</strong> to capture spatial hierarchies in the data.</p>
<h2 id="Architecture-of-a-CNN"><a href="#Architecture-of-a-CNN" class="headerlink" title="Architecture of a CNN"></a>Architecture of a CNN</h2><p>A typical CNN consists of the following layers:<br><img src="/blog/./img/ML8-1.png" alt="https://www.sciencedirect.com/topics/mathematics/pooling-layer"></p>
<ol>
<li><p><strong>Convolutional Layer</strong><br>The convolutional layer applies a set of learnable filters to the input image, extracting important features like edges, textures, and patterns. Mathematically, the convolution operation is defined as:</p>
<p>$$ (I * K)(x, y) &#x3D; \sum_{m}\sum_{n} I(m, n)K(x - m, y - n) $$</p>
<p>where:</p>
<ul>
<li>$I$ is the input image,</li>
<li>$K$ is the filter (kernel),</li>
<li>$(x, y)$ represents spatial coordinates.</li>
</ul>
</li>
</ol>
<p><img src="/blog/./img/ML8-2.gif" alt="https://medium.com/@timothy_terati/image-convolution-filtering-a54dce7c786b"></p>
<ol start="2">
<li><p><strong>Activation Function (ReLU)</strong><br>The Rectified Linear Unit (ReLU) introduces non-linearity into the model by applying:</p>
<p>$$ f(x) &#x3D; \max(0, x) $$</p>
<p>This helps the network learn complex patterns.</p>
</li>
<li><p><strong>Pooling Layer</strong><br>The pooling layer reduces the spatial dimensions of the feature maps, helping to decrease computational complexity. The most common type is max pooling:</p>
<p>$$ P(x, y) &#x3D; \max_{i, j} F(x+i, y+j) $$</p>
<p>where $F(x, y)$ represents the feature map values.</p>
</li>
<li><p><strong>Fully Connected Layer</strong><br>After several convolutional and pooling layers, the output is <strong>flattened</strong> and passed through <strong>fully connected layers</strong> to make final predictions.</p>
</li>
</ol>
<h2 id="Training-a-CNN"><a href="#Training-a-CNN" class="headerlink" title="Training a CNN"></a>Training a CNN</h2><p>Training a CNN involves minimizing a loss function using an optimization algorithm like stochastic gradient descent (SGD). The loss function, often cross-entropy loss, is given by:</p>
<p>$$ L &#x3D; -\sum_{i} y_i \log(\hat{y_i}) $$</p>
<p>where:</p>
<ul>
<li>$y_i$ is the true label,</li>
<li>$\hat{y_i}$ is the predicted probability.</li>
</ul>
<p>Backpropagation and gradient descent adjust the weights to minimize this loss iteratively.</p>
<h2 id="Applications-of-CNNs"><a href="#Applications-of-CNNs" class="headerlink" title="Applications of CNNs"></a>Applications of CNNs</h2><p>CNNs have a wide range of applications, including:</p>
<ul>
<li><strong>Image Classification</strong> (e.g., recognizing objects in images)</li>
<li><strong>Object Detection</strong> (e.g., detecting pedestrians in autonomous driving)</li>
<li><strong>Medical Image Analysis</strong> (e.g., detecting tumors in X-rays)</li>
<li><strong>Facial Recognition</strong> (e.g., biometric authentication)</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-09T21:03:49.000Z" title="2/9/2025, 9:03:49 PM">2025-02-09</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-02-16T02:45:33.363Z" title="2/16/2025, 2:45:33 AM">2025-02-16</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/09/Transformer-1/">Transformer ( Part 1: Word Embedding )</a></p><div class="content"><p>Word Embedding is one of the most fundamental techniques in Natural Language Processing (NLP). It represents words as continuous vectors in a high-dimensional space, capturing semantic relationships between them.  </p>
<h2 id="Why-Do-We-Need-Word-Embeddings"><a href="#Why-Do-We-Need-Word-Embeddings" class="headerlink" title="Why Do We Need Word Embeddings?"></a>Why Do We Need Word Embeddings?</h2><p>Before word embeddings, one common method to represent words was <strong>One-Hot Encoding</strong>. In this approach, each word is represented as a high-dimensional sparse vector.  </p>
<p>For example, if our vocabulary has 10,000 words, we encode each word as:<br>$$<br>\text{dog} &#x3D; [0, 1, 0, 0, \dots, 0]<br>$$<br>However, this method has significant drawbacks:  </p>
<ol>
<li><strong>High dimensionality</strong> – A large vocabulary results in enormous vectors.  </li>
<li><strong>No semantic similarity</strong> – “dog” and “cat” are conceptually related, but their one-hot vectors are completely different.</li>
</ol>
<p>Word embeddings solve these issues by <strong>learning low-dimensional, dense representations</strong> that encode semantic relationships between words.  </p>
<hr>
<h2 id="Word-Embeddings-and-Dot-Product-A-Geometric-Perspective"><a href="#Word-Embeddings-and-Dot-Product-A-Geometric-Perspective" class="headerlink" title="Word Embeddings and Dot Product: A Geometric Perspective"></a>Word Embeddings and Dot Product: A Geometric Perspective</h2><p>Word embeddings map words into a <strong>vector space</strong>, where semantically similar words are placed close to each other.  </p>
<h3 id="What-is-the-Dot-Product"><a href="#What-is-the-Dot-Product" class="headerlink" title="What is the Dot Product?"></a><strong>What is the Dot Product?</strong></h3><p>For two vectors $\mathbf{A}$ and $\mathbf{B}$, the <strong>dot product</strong> is defined as:<br>$$<br>\mathbf{A} \cdot \mathbf{B} &#x3D; | \mathbf{A} | | \mathbf{B} | \cos(\theta)<br>$$<br>where:  </p>
<ul>
<li>$| \mathbf{A} |$ and $| \mathbf{B} |$ are the magnitudes (lengths) of the vectors  </li>
<li>$| \mathbf{A} |$ and $| \mathbf{B} |$ are the magnitudes (lengths) of the vectors  </li>
<li>$\theta$ is the angle between them</li>
</ul>
<h4 id="Why-is-This-Important-in-Word-Embeddings"><a href="#Why-is-This-Important-in-Word-Embeddings" class="headerlink" title="Why is This Important in Word Embeddings?"></a><strong>Why is This Important in Word Embeddings?</strong></h4><ul>
<li><strong>When $\theta$ is small</strong>, $\cos(\theta)$ is large, meaning the words are <strong>similar</strong>.  </li>
<li><strong>When $\theta$ is large</strong>, $\cos(\theta)$ is small or negative, meaning the words are <strong>dissimilar</strong>.</li>
</ul>
<p>Thus, in Word2Vec, the similarity between two words is determined by their <strong>dot product</strong>, which aligns with how we measure relationships in a semantic space.  </p>
<hr>
<h2 id="Word2Vec-CBOW-and-Skip-Gram"><a href="#Word2Vec-CBOW-and-Skip-Gram" class="headerlink" title="Word2Vec: CBOW and Skip-Gram"></a>Word2Vec: CBOW and Skip-Gram</h2><p>Word2Vec is an algorithm for learning word embeddings. It has two main architectures:  </p>
<ol>
<li><strong>Continuous Bag of Words (CBOW)</strong> – Predicts a word given its surrounding context.  </li>
<li><strong>Skip-Gram</strong> – Predicts context words given a target word.</li>
</ol>
<h3 id="CBOW-Model"><a href="#CBOW-Model" class="headerlink" title="CBOW Model"></a><strong>CBOW Model</strong></h3><p>Given surrounding words, we predict the center word. The model uses:<br>$$<br>\mathbf{h} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} \mathbf{x}_i<br>$$<br>where $\mathbf{x}_i$ are the input word vectors.  </p>
<p>The probability of the target word $w_t$ is given by the <strong>Softmax function</strong>:<br>$$<br>P(w_t | \text{context}) &#x3D; \frac{\exp(\mathbf{v_{w_t}} \cdot \mathbf{h})}{\sum_{w \in V} \exp(\mathbf{v_w} \cdot \mathbf{h})}<br>$$  </p>
<h3 id="Skip-Gram-Model"><a href="#Skip-Gram-Model" class="headerlink" title="Skip-Gram Model"></a><strong>Skip-Gram Model</strong></h3><p>Instead of predicting a word from its context, Skip-Gram predicts the <strong>context words given a target word</strong>:<br>$$<br>P(w_c | w_t) &#x3D; \frac{\exp(\mathbf{v_{w_c}} \cdot \mathbf{v_{w_t}})}{\sum_{w \in V} \exp(\mathbf{v_w} \cdot \mathbf{v_{w_t}})}<br>$$  </p>
<p>The loss function for Skip-Gram is:<br>$$<br>L &#x3D; -\sum_{t} \sum_{-n \leq j \leq n, j \neq 0} \log P(w_{t+j} | w_t)<br>$$  </p>
<hr>
<h2 id="Understanding-Softmax-and-Exponential-Function"><a href="#Understanding-Softmax-and-Exponential-Function" class="headerlink" title="Understanding Softmax and Exponential Function"></a>Understanding Softmax and Exponential Function</h2><h3 id="What-is-Softmax"><a href="#What-is-Softmax" class="headerlink" title="What is Softmax?"></a><strong>What is Softmax?</strong></h3><p>Softmax converts raw scores into probabilities. Given a vector $\mathbf{z}$, Softmax is defined as:<br>$$<br>\text{Softmax}(z_i) &#x3D; \frac{\exp(z_i)}{\sum_{j} \exp(z_j)}<br>$$  </p>
<p>This ensures:  </p>
<ul>
<li><strong>Non-negative outputs</strong> (since $\exp(x) &gt; 0$ for all $x$).  </li>
<li><strong>Probabilities sum to 1</strong> (as a requirement for classification).</li>
</ul>
<p>The <strong>exponential function</strong> $\exp(x) &#x3D; e^x$ grows rapidly as $x$ increases. It is useful because:  </p>
<ul>
<li>It ensures probabilities never become negative.  </li>
<li>It amplifies differences between scores, making classification more confident.</li>
</ul>
<hr>
<h2 id="Negative-Sampling-Efficient-Training"><a href="#Negative-Sampling-Efficient-Training" class="headerlink" title="Negative Sampling: Efficient Training"></a>Negative Sampling: Efficient Training</h2><p>Since computing Softmax over the <strong>entire vocabulary</strong> is expensive, we use <strong>Negative Sampling</strong>, which simplifies the loss function to:<br>$$<br>L &#x3D; \log \sigma (\mathbf{v_{w_t}} \cdot \mathbf{v_{w_c}}) + \sum_{i&#x3D;1}^{k} \log \sigma (-\mathbf{v_{w_t}} \cdot \mathbf{v_{w_{\text{neg}_i}}})<br>$$<br>where $\sigma(x) &#x3D; \frac{1}{1 + e^{-x}}$ is the <strong>Sigmoid function</strong>.  </p>
<p>Negative Sampling selects a few “negative examples” to contrast with positive word pairs, making training much faster.  </p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-06T21:43:39.000Z" title="2/6/2025, 9:43:39 PM">2025-02-06</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-02-16T02:45:33.363Z" title="2/16/2025, 2:45:33 AM">2025-02-16</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/06/Machine-Learning-7/">About Machine Learning ( Part 7: Artificial Neural Network )</a></p><div class="content"><h1 id="Bayes’-theorem"><a href="#Bayes’-theorem" class="headerlink" title="Bayes’ theorem"></a>Bayes’ theorem</h1><p>$$<br>P(y|X) &#x3D; \frac{P(X|y) P(y)}{P(X)}<br>$$</p>
<p>where:</p>
<ul>
<li>$P(y|X)$: Posterior probability of class $y$ given input $X$.</li>
<li>$P(X|y)$: Likelihood of seeing $X$ if the class is $y$.</li>
<li>$P(y)$: Prior probability of class $y$.</li>
<li>$P(X)$: Total probability of $X$ (normalization factor).</li>
</ul>
<h2 id="Bayes-Network-Bayesian-Network-BN"><a href="#Bayes-Network-Bayesian-Network-BN" class="headerlink" title="Bayes Network (Bayesian Network, BN)"></a>Bayes Network (Bayesian Network, BN)</h2><p>A <strong>Bayesian network (BN)</strong> is a <strong>graphical model</strong> representing probabilistic dependencies between variables. It consists of:</p>
<ul>
<li><strong>Nodes:</strong> Represent variables (e.g., symptoms, diseases).</li>
<li><strong>Edges:</strong> Represent conditional dependencies.</li>
</ul>
<p><img src="/blog/./img/ML7-1.png"></p>
<h2 id="Bayesian-Inference-in-BN"><a href="#Bayesian-Inference-in-BN" class="headerlink" title="Bayesian Inference in BN"></a>Bayesian Inference in BN</h2><p>Using Bayes’ rule, we can infer probabilities, such as:</p>
<p>$$<br>P(F|T, C) &#x3D; \frac{P(T, C | F) P(F)}{P(T, C)}<br>$$</p>
<p>Bayesian networks are widely used in <strong>medical diagnosis, fraud detection, and AI decision-making</strong>.</p>
<h1 id="Artificial-Neural-Network-ANN"><a href="#Artificial-Neural-Network-ANN" class="headerlink" title="Artificial Neural Network (ANN)"></a>Artificial Neural Network (ANN)</h1><h2 id="The-Perceptron"><a href="#The-Perceptron" class="headerlink" title="The Perceptron"></a>The Perceptron</h2><p>A <strong>perceptron</strong> is a simple artificial neuron that performs binary classification. It computes a weighted sum of inputs and applies an activation function:</p>
<p>$$<br>y &#x3D; \begin{cases}<br>1, &amp; \text{if } w \cdot X + b &gt; 0 \newline<br>0, &amp; \text{otherwise}<br>\end{cases}<br>$$</p>
<p>where:</p>
<ul>
<li>$X$ is the input vector.</li>
<li>$w$ is the weight vector.</li>
<li>$b$ is the bias term.</li>
</ul>
<h2 id="Learning-in-Perceptrons"><a href="#Learning-in-Perceptrons" class="headerlink" title="Learning in Perceptrons"></a>Learning in Perceptrons</h2><p>The perceptron updates its weights using a simple rule:</p>
<p>$$<br>w \leftarrow w + \Delta w<br>$$</p>
<p>where:</p>
<p>$$<br>\Delta w &#x3D; \eta (y_{\text{true}} - y_{\text{pred}}) X<br>$$</p>
<ul>
<li>$\eta$ is the learning rate.</li>
<li>The perceptron <strong>adjusts weights</strong> only when it makes an error.</li>
</ul>
<p><strong>Limitations of the Perceptron</strong>:</p>
<ul>
<li>Can only solve <strong>linearly separable problems</strong> (e.g., AND, OR gates).</li>
<li><strong>Fails for XOR problems</strong>, motivating more advanced learning rules.</li>
<li>So <strong>activation funtion</strong> are usually used.</li>
</ul>
<h2 id="Delta-Rule"><a href="#Delta-Rule" class="headerlink" title="Delta Rule"></a>Delta Rule</h2><p>The <strong>Delta Rule</strong> is a gradient descent learning rule used in <strong>single-layer perceptrons</strong> and neural networks to minimize error and update weights. The main idea behind this rule is to adjust the weights based on the <strong>gradient of the error</strong> to minimize the loss function (often the Mean Squared Error, MSE). The Delta Rule is a specific case of the <strong>Gradient Descent</strong> method.</p>
<p>Assume the output of a perceptron is calculated by the following formula:</p>
<p>$$<br>y &#x3D; f(w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b)<br>$$</p>
<p>Where:</p>
<ul>
<li>$x_i$ is the input value,</li>
<li>$w_i$ is the corresponding weight,</li>
<li>$b$ is the bias,</li>
<li>$f(\cdot)$ is the activation function (usually a linear or Sigmoid function),</li>
<li>$y$ is the output,</li>
<li>$t$ is the target label (the true value),</li>
<li>$e &#x3D; t - y$ is the error (difference between predicted and true output).</li>
</ul>
<p>The <strong>Delta Rule</strong> updates the weights as follows:</p>
<p>$$<br>\Delta w_i &#x3D; \eta (t - y) x_i<br>$$</p>
<p>Where:</p>
<ul>
<li>$\eta$ is the <strong>learning rate</strong> that controls the step size of the updates,</li>
<li>$(t - y)$ is the error,</li>
<li>$x_i$ is the input used to adjust the corresponding weight.</li>
</ul>
<p>The weight update rule becomes:</p>
<p>$$<br>w_i \leftarrow w_i + \Delta w_i<br>$$</p>
<h3 id="Derivation-of-the-Delta-Rule"><a href="#Derivation-of-the-Delta-Rule" class="headerlink" title="Derivation of the Delta Rule"></a>Derivation of the Delta Rule</h3><h4 id="The-Error-Function-Mean-Squared-Error"><a href="#The-Error-Function-Mean-Squared-Error" class="headerlink" title="The Error Function (Mean Squared Error)"></a>The Error Function (Mean Squared Error)</h4><p>To understand how the Delta Rule works, we first need to define the error function. The <strong>Mean Squared Error (MSE)</strong> is commonly used as the error metric in neural networks, especially for regression tasks. It measures the difference between the target output $t$ and the predicted output $y$:</p>
<p>$$<br>E &#x3D; \frac{1}{2} (t - y)^2<br>$$</p>
<p>Where:</p>
<ul>
<li>$t$ is the target value (the true label),</li>
<li>$y$ is the predicted output from the neural network.</li>
</ul>
<p>The reason we use a factor of $\frac{1}{2}$ is to simplify the derivative when we apply the chain rule during weight updates.</p>
<h4 id="The-Goal-Weight-Update"><a href="#The-Goal-Weight-Update" class="headerlink" title="The Goal: Weight Update"></a>The Goal: Weight Update</h4><p>The goal of the Delta Rule is to update the weights so that the error $E$ is minimized. We do this by adjusting the weights in the direction that reduces the error. To achieve this, we compute the gradient (partial derivative) of the error function with respect to the weights $w_i$.</p>
<p>The output $y$ of a single output unit in the network is determined by:</p>
<p>$$<br>y &#x3D; f(net)<br>$$</p>
<p>Where:</p>
<ul>
<li>$net &#x3D; \sum_{i} w_i x_i + b$ is the weighted sum of inputs, </li>
<li>$f(\cdot)$ is the activation function applied to the weighted sum (in this case, we’ll focus on the <strong>Sigmoid</strong> function).</li>
</ul>
<p>To update the weights, we need the partial derivative of the error $E$ with respect to each weight $w_i$. This is done using the chain rule:</p>
<p>$$<br>\frac{\partial E}{\partial w_i} &#x3D; \frac{\partial E}{\partial y} \cdot \frac{\partial y}{\partial w_i}<br>$$</p>
<h4 id="Compute-frac-partial-E-partial-y"><a href="#Compute-frac-partial-E-partial-y" class="headerlink" title="Compute $\frac{\partial E}{\partial y}$"></a>Compute $\frac{\partial E}{\partial y}$</h4><p>The first term we need is the derivative of the error with respect to the output $y$. Since $E$ is the squared error, we can differentiate:</p>
<p>$$<br>\frac{\partial E}{\partial y} &#x3D; -(t - y)<br>$$</p>
<p>This represents how much the error changes with respect to the output $y$.</p>
<h4 id="Compute-frac-partial-y-partial-w-i"><a href="#Compute-frac-partial-y-partial-w-i" class="headerlink" title="Compute $\frac{\partial y}{\partial w_i}$"></a>Compute $\frac{\partial y}{\partial w_i}$</h4><p>Next, we calculate the derivative of the output $y$ with respect to each weight $w_i$. The output $y$ is the result of applying the activation function $f$ to the weighted sum $net$:</p>
<p>$$<br>y &#x3D; f(net)<br>$$</p>
<p>Using the chain rule, we get:</p>
<p>$$<br>\frac{\partial y}{\partial w_i} &#x3D; \frac{\partial f(net)}{\partial net} \cdot \frac{\partial net}{\partial w_i}<br>$$</p>
<p>Where:</p>
<ul>
<li>$\frac{\partial f(net)}{\partial net} &#x3D; f’(net)$ is the derivative of the activation function with respect to the net input,</li>
<li>$\frac{\partial net}{\partial w_i} &#x3D; x_i$, since $net &#x3D; \sum w_i x_i + b$.</li>
</ul>
<p>Thus:</p>
<p>$$<br>\frac{\partial y}{\partial w_i} &#x3D; f’(net) \cdot x_i<br>$$</p>
<h4 id="Combine-Terms-for-the-Gradient"><a href="#Combine-Terms-for-the-Gradient" class="headerlink" title="Combine Terms for the Gradient"></a>Combine Terms for the Gradient</h4><p>Now, we combine the terms to compute the gradient of the error with respect to the weight $w_i$:</p>
<p>$$<br>\frac{\partial E}{\partial w_i} &#x3D; -(t - y) \cdot f’(net) \cdot x_i<br>$$</p>
<p>This expression tells us how much to adjust each weight $w_i$ to minimize the error.</p>
<h4 id="Derivative-of-the-Sigmoid-Activation-Function"><a href="#Derivative-of-the-Sigmoid-Activation-Function" class="headerlink" title="Derivative of the Sigmoid Activation Function"></a>Derivative of the Sigmoid Activation Function</h4><p>For the <strong>Sigmoid activation function</strong>, the output $o$ is given by:</p>
<p>$$<br>o &#x3D; f(net) &#x3D; \frac{1}{1 + e^{-net}}<br>$$</p>
<p>The derivative of the Sigmoid function with respect to the net input $net$ is:</p>
<p>$$<br>f’(net) &#x3D; o(1 - o)<br>$$</p>
<p>Where $o$ is the output of the Sigmoid function.</p>
<h4 id="The-Final-Weight-Update-Rule"><a href="#The-Final-Weight-Update-Rule" class="headerlink" title="The Final Weight Update Rule"></a>The Final Weight Update Rule</h4><p>We can now substitute the derivative of the Sigmoid function into the weight update formula. The Delta Rule for updating weights is:</p>
<p>$$<br>\Delta w_i &#x3D; \eta (t - y) \cdot o \cdot (1 - o) \cdot x_i<br>$$</p>
<p>Where:</p>
<ul>
<li>$\eta$ is the learning rate, which controls how much the weights are adjusted at each step,</li>
<li>$(t - y)$ is the error between the target and predicted output,</li>
<li>$o(1 - o)$ is the derivative of the Sigmoid activation function,</li>
<li>$x_i$ is the input corresponding to the weight $w_i$.</li>
</ul>
<p>Finally, the weights are updated as follows:</p>
<p>$$<br>w_i \leftarrow w_i + \Delta w_i<br>$$</p>
<p>This update ensures that the weights move in the direction that minimizes the error.</p>
<h2 id="The-Difference-in-the-Calculation-of-delta-k-for-Output-and-Hidden-Layers"><a href="#The-Difference-in-the-Calculation-of-delta-k-for-Output-and-Hidden-Layers" class="headerlink" title="The Difference in the Calculation of $\delta_k$ for Output and Hidden Layers"></a>The Difference in the Calculation of $\delta_k$ for Output and Hidden Layers</h2><p>The calculation of the error signal $\delta_k$ is different in the <strong>output layer</strong> and the <strong>hidden layer</strong> because:</p>
<ul>
<li><strong>Output layer</strong> error is directly related to the target error, and we can compute the gradient directly.</li>
<li><strong>Hidden layer</strong> error is not directly related to the target, so we need to <strong>propagate the error backwards</strong> from downstream layers (closer to the output layer).</li>
</ul>
<h3 id="Output-Layer’s-delta-k-Direct-Calculation-from-Error"><a href="#Output-Layer’s-delta-k-Direct-Calculation-from-Error" class="headerlink" title="Output Layer’s $\delta_k$: Direct Calculation from Error"></a>Output Layer’s $\delta_k$: Direct Calculation from Error</h3><p>In the <strong>output layer</strong>, the error can be directly calculated because the output neurons compare their output values $o_k$ directly with the target values $y_k$. Thus, the error signal $\delta_k$ is <strong>calculated directly based on the loss function</strong>.</p>
<p>For the <strong>Mean Squared Error (MSE)</strong> loss function:</p>
<p>$$<br>E &#x3D; \frac{1}{2} \sum_k (y_k - o_k)^2<br>$$</p>
<p>We take the partial derivative with respect to $net_k$:</p>
<p>$$<br>\delta_k &#x3D; \frac{\partial E}{\partial net_k} &#x3D; \frac{\partial E}{\partial o_k} \cdot \frac{\partial o_k}{\partial net_k}<br>$$</p>
<p>Where:</p>
<ul>
<li>$\frac{\partial E}{\partial o_k} &#x3D; -(y_k - o_k)$ (the error term),</li>
<li>$\frac{\partial o_k}{\partial net_k}$ depends on the activation function (e.g., Sigmoid: $o_k(1 - o_k)$).</li>
</ul>
<p>Thus, for the <strong>Sigmoid activation function</strong>:</p>
<p>$$<br>\delta_k &#x3D; -(y_k - o_k) \cdot o_k (1 - o_k)<br>$$</p>
<p>This calculation is <strong>specific to the output layer</strong> because the error term $y_k - o_k$ is directly available.</p>
<h3 id="Hidden-Layer’s-delta-h-Backpropagating-Error-from-Downstream"><a href="#Hidden-Layer’s-delta-h-Backpropagating-Error-from-Downstream" class="headerlink" title="Hidden Layer’s $\delta_h$: Backpropagating Error from Downstream"></a>Hidden Layer’s $\delta_h$: Backpropagating Error from Downstream</h3><p>In the <strong>hidden layer</strong>, the error cannot be directly calculated because the hidden neurons do not directly compare their outputs with target values. So, how do we know how much a hidden neuron contributes to the final error?</p>
<p><strong>The answer is: The error signal propagates backward from downstream layers!</strong></p>
<h4 id="What-are-downstream-layers"><a href="#What-are-downstream-layers" class="headerlink" title="What are downstream layers?"></a>What are downstream layers?</h4><ul>
<li><strong>Neural networks perform forward propagation</strong> for computing outputs, but the error is propagated <strong>backward</strong>.</li>
<li><strong>Hidden neurons influence multiple output neurons</strong>, so their error needs to be propagated back from those output neurons.</li>
<li>“Downstream” refers to layers closer to the <strong>output layer</strong>, and “upstream” refers to layers closer to the <strong>input layer</strong>.</li>
</ul>
<h4 id="Calculating-the-Hidden-Layer’s-Error-Signal-delta-h"><a href="#Calculating-the-Hidden-Layer’s-Error-Signal-delta-h" class="headerlink" title="Calculating the Hidden Layer’s Error Signal $\delta_h$"></a>Calculating the Hidden Layer’s Error Signal $\delta_h$</h4><p>For a hidden layer neuron $h$, there is no direct error, so its gradient comes from <strong>downstream layers</strong> (the output layer or deeper hidden layers).</p>
<p>Using <strong>the Chain Rule</strong>, we calculate:</p>
<p>$$<br>\delta_h &#x3D; \sum_k \left( \delta_k \cdot \omega_{kh} \right) \cdot o_h (1 - o_h)<br>$$</p>
<p>Where:</p>
<ul>
<li>$\delta_k$ is the downstream (output layer) error signal,</li>
<li>$\omega_{kh}$ is the weight from the hidden layer neuron $h$ to the output layer neuron $k$,</li>
<li>$o_h(1 - o_h)$ is the derivative of the activation function for the hidden layer neuron.</li>
<li>$\sum_k (\delta_k \cdot \omega_{kh})$: If the hidden layer neuron $h$ is connected to multiple output neurons $k$, its error is the <strong>weighted sum of all these error signals</strong>.</li>
</ul>
<h2 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h2><p>Backpropagation is the core algorithm behind training <strong>multi-layer neural networks</strong>. It efficiently computes the <strong>gradient of the loss function</strong> with respect to the network’s weights, enabling the network to <strong>learn</strong> from data through gradient descent.</p>
<h3 id="Backpropagation-Algorithm"><a href="#Backpropagation-Algorithm" class="headerlink" title="Backpropagation Algorithm"></a>Backpropagation Algorithm</h3><p><strong>Step 1: Initialize the network</strong></p>
<ul>
<li>Create a <strong>feed-forward network</strong> with:<ul>
<li>$n_{in}$ input neurons</li>
<li>$n_h$ hidden neurons</li>
<li>$n_{out}$ output neurons</li>
</ul>
</li>
<li>Assign <strong>random small weights</strong> (e.g., between $-0.05$ and $0.05$) to break symmetry.</li>
</ul>
<p><strong>Step 2: Forward Propagation</strong><br>For each training example $\mathbf{x}$:</p>
<ol>
<li>Compute the weighted sum of inputs for each neuron.</li>
<li>Apply an <strong>activation function</strong> (e.g., Sigmoid) to get the output.</li>
</ol>
<p><strong>Step 3: Compute Errors</strong></p>
<ol>
<li><p>Compute the <strong>error at the output layer</strong>:<br>$$<br>\delta_j &#x3D; \sigma_j (1 - \sigma_j) (t_j - out_j)<br>$$<br>where:</p>
<ul>
<li>$\sigma_j$ is the output of the neuron</li>
<li>$t_j$ is the target value</li>
<li>$out_j$ is the actual output</li>
</ul>
</li>
<li><p>Compute the <strong>error at the hidden layers</strong> by propagating errors backward:<br> $$<br> \delta_j &#x3D; \sigma_j (1 - \sigma_j) \sum_{k \in \text{downstream}} w_{kj} \delta_k<br> $$</p>
</li>
</ol>
<p><strong>Step 4: Update Weights</strong><br>Each weight $w_{ij}$ is updated using gradient descent:<br>$$<br>w_{ij} \leftarrow w_{ij} + \eta \delta_j x_i<br>$$<br>where $\eta$ is the <strong>learning rate</strong>.</p>
<p><img src="/blog/./img/ML7-2.png" alt="https://www.science.org/doi/10.1126/sciadv.ado8999"></p>
<h3 id="Numerical-Example-of-Backpropagation"><a href="#Numerical-Example-of-Backpropagation" class="headerlink" title="Numerical Example of Backpropagation"></a>Numerical Example of Backpropagation</h3><p>Let’s go through an example using a <strong>small network</strong>.</p>
<h4 id="Network-Structure"><a href="#Network-Structure" class="headerlink" title="Network Structure"></a><strong>Network Structure</strong></h4><ul>
<li><strong>Input layer:</strong> 2 neurons ($x_1, x_2$)</li>
<li><strong>Hidden layer:</strong> 2 neurons ($h_1, h_2$)</li>
<li><strong>Output layer:</strong> 1 neuron ($o$)</li>
<li>Activation function: <strong>Sigmoid</strong></li>
</ul>
<h4 id="Given-Initial-Values"><a href="#Given-Initial-Values" class="headerlink" title="Given Initial Values"></a><strong>Given Initial Values</strong></h4><ul>
<li><strong>Inputs:</strong> $x_1 &#x3D; 0.05, x_2 &#x3D; 0.10$</li>
<li><strong>Targets:</strong> $t &#x3D; 0.01$</li>
<li><strong>Weights:</strong><ul>
<li>Input to Hidden:<ul>
<li>$w_{1,1} &#x3D; 0.15$, $w_{1,2} &#x3D; 0.20$</li>
<li>$w_{2,1} &#x3D; 0.25$, $w_{2,2} &#x3D; 0.30$</li>
</ul>
</li>
<li>Hidden to Output:<ul>
<li>$w_{h1,o} &#x3D; 0.40$, $w_{h2,o} &#x3D; 0.45$</li>
</ul>
</li>
</ul>
</li>
<li><strong>Biases:</strong> Assume <strong>0</strong> for simplicity.</li>
<li><strong>Learning Rate:</strong> $\eta &#x3D; 0.5$</li>
</ul>
<h4 id="Step-1-Forward-Pass"><a href="#Step-1-Forward-Pass" class="headerlink" title="Step 1: Forward Pass"></a><strong>Step 1: Forward Pass</strong></h4><p>Compute the net input and output of the hidden neurons:</p>
<p>$$<br>net_{h1} &#x3D; w_{1,1}x_1 + w_{2,1}x_2 &#x3D; (0.15)(0.05) + (0.25)(0.10) &#x3D; 0.0125<br>$$</p>
<p>$$<br>\sigma_{h1} &#x3D; \frac{1}{1 + e^{-net_{h1}}} &#x3D; \frac{1}{1 + e^{-0.0125}} \approx 0.5031<br>$$</p>
<p>Similarly, for $h_2$:</p>
<p>$$<br>net_{h2} &#x3D; (0.20)(0.05) + (0.30)(0.10) &#x3D; 0.0175<br>$$</p>
<p>$$<br>\sigma_{h2} &#x3D; \frac{1}{1 + e^{-0.0175}} \approx 0.5044<br>$$</p>
<p>For the output neuron:</p>
<p>$$<br>net_o &#x3D; w_{h1,o} \sigma_{h1} + w_{h2,o} \sigma_{h2} &#x3D; (0.40)(0.5031) + (0.45)(0.5044) &#x3D; 0.4009<br>$$</p>
<p>$$<br>out_o &#x3D; \frac{1}{1 + e^{-net_o}} &#x3D; \frac{1}{1 + e^{-0.4009}} \approx 0.5988<br>$$</p>
<h4 id="Step-2-Compute-Error"><a href="#Step-2-Compute-Error" class="headerlink" title="Step 2: Compute Error"></a><strong>Step 2: Compute Error</strong></h4><p>$$<br>E &#x3D; \frac{1}{2} (t - out_o)^2 &#x3D; \frac{1}{2} (0.01 - 0.5988)^2 &#x3D; 0.174<br>$$</p>
<h4 id="Step-3-Backpropagation"><a href="#Step-3-Backpropagation" class="headerlink" title="Step 3: Backpropagation"></a><strong>Step 3: Backpropagation</strong></h4><p><strong>Error at Output Layer</strong><br>$$<br>\delta_o &#x3D; out_o(1 - out_o)(t - out_o)<br>$$<br>$$<br>\delta_o &#x3D; (0.5988)(1 - 0.5988)(0.01 - 0.5988) &#x3D; -0.1432<br>$$</p>
<p><strong>Error at Hidden Layer</strong><br>For $h_1$:</p>
<p>$$<br>\delta_{h1} &#x3D; \sigma_{h1} (1 - \sigma_{h1}) (w_{h1,o} \delta_o)<br>$$<br>$$<br>\delta_{h1} &#x3D; (0.5031)(1 - 0.5031)(0.40)(-0.1432) &#x3D; -0.0143<br>$$</p>
<p>Similarly, for $h_2$:</p>
<p>$$<br>\delta_{h2} &#x3D; (0.5044)(1 - 0.5044)(0.45)(-0.1432) &#x3D; -0.0161<br>$$</p>
<h4 id="Step-4-Weight-Updates"><a href="#Step-4-Weight-Updates" class="headerlink" title="Step 4: Weight Updates"></a><strong>Step 4: Weight Updates</strong></h4><p>$$<br>w_{h1,o} \leftarrow w_{h1,o} + \eta \delta_o \sigma_{h1}<br>$$<br>$$<br>&#x3D; 0.40 + (0.5)(-0.1432)(0.5031) &#x3D; 0.364<br>$$</p>
<p>Similarly, other weights are updated.</p>
<h3 id="The-Exploding-and-Vanishing-Gradient-Problem"><a href="#The-Exploding-and-Vanishing-Gradient-Problem" class="headerlink" title="The Exploding and Vanishing Gradient Problem"></a>The Exploding and Vanishing Gradient Problem</h3><p>Backpropagation works well for shallow networks, but <strong>deep networks</strong> suffer from two major problems:</p>
<p><strong>Vanishing Gradient</strong></p>
<ul>
<li>In deep networks, gradients become <strong>exponentially smaller</strong> as they propagate backward.</li>
<li>Since weights are updated using these gradients, <strong>early layers stop learning</strong>.</li>
<li>Happens when using <strong>Sigmoid or Tanh</strong> activations because their derivatives are between <strong>0 and 1</strong>.</li>
</ul>
<p><strong>Exploding Gradient</strong></p>
<ul>
<li>If weights are large, gradients <strong>explode</strong>, leading to <strong>unstable training</strong>.</li>
<li><strong>Weight updates become extremely large</strong>, causing the model to diverge.</li>
</ul>
<p><strong>Solutions</strong></p>
<ol>
<li><strong>ReLU Activation Function</strong>: Unlike Sigmoid, ReLU has a derivative of <strong>1</strong> for positive inputs, preventing vanishing gradients.</li>
<li><strong>Batch Normalization</strong>: Normalizes activations, keeping them in a stable range.</li>
<li><strong>Gradient Clipping</strong>: Limits the gradient value to prevent explosion.</li>
<li><strong>Xavier&#x2F;He Initialization</strong>: Properly initializes weights to keep gradients stable.</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-02T18:20:57.000Z" title="2/2/2025, 6:20:57 PM">2025-02-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-02-16T02:45:33.363Z" title="2/16/2025, 2:45:33 AM">2025-02-16</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/02/Machine-Learning-6/">About Machine Learning ( Part 6: KNN vs. K-means )</a></p><div class="content"><p>In machine learning, <strong>K-Nearest Neighbors (KNN)</strong> and <strong>K-means Clustering</strong> are two commonly used algorithms. Despite their similar names, they serve <strong>different purposes</strong> and have <strong>distinct working principles</strong>.  </p>
<h1 id="KNN-K-Nearest-Neighbors"><a href="#KNN-K-Nearest-Neighbors" class="headerlink" title="KNN (K-Nearest Neighbors)"></a>KNN (K-Nearest Neighbors)</h1><p>KNN is a <strong>supervised learning</strong> algorithm used for <strong>classification</strong> and <strong>regression</strong> tasks.  </p>
<p>The core idea of KNN is:</p>
<blockquote>
<p>Given a new data point, find the <strong>K</strong> most similar instances in the training dataset (neighbors) and use them to predict the output.</p>
</blockquote>
<p>KNN is a <strong>lazy learning</strong> algorithm, meaning it does not require a training phase. Instead, it directly classifies or predicts based on stored data.</p>
<p>KNN follows these steps:</p>
<ol>
<li>Compute the <strong>distance</strong> between the new data point and all training samples (e.g., using Euclidean distance).</li>
<li>Select the <strong>K nearest neighbors</strong>.</li>
<li>Predict the result:<ul>
<li><strong>For classification</strong>: Use a majority vote among the K neighbors.</li>
<li><strong>For regression</strong>: Take the average of the K neighbors.</li>
</ul>
</li>
</ol>
<h2 id="Mathematical-Formulation"><a href="#Mathematical-Formulation" class="headerlink" title="Mathematical Formulation"></a>Mathematical Formulation</h2><p>For two points $A(x_1, y_1)$ and $B(x_2, y_2)$, the <strong>Euclidean distance</strong> is:</p>
<p>$$<br>d(A, B) &#x3D; \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}<br>$$</p>
<p>In <strong>n-dimensional space</strong>, it generalizes to:</p>
<p>$$<br>d(A, B) &#x3D; \sqrt{\sum_{i&#x3D;1}^{n} (x_i - y_i)^2}<br>$$</p>
<p>where:</p>
<ul>
<li>$x_i$ and $y_i$ are the coordinates of points $A$ and $B$ in dimension $i$.</li>
</ul>
<p>If the K nearest neighbors have labels $y_1, y_2, …, y_K$, then the predicted label $\hat{y}$ is:</p>
<p>$$<br>\hat{y} &#x3D; \arg\max_{c} \sum_{i&#x3D;1}^{K} \mathbb{I}(y_i &#x3D; c)<br>$$</p>
<p>where:</p>
<ul>
<li>$\mathbb{I}(\cdot)$ is an indicator function, returning 1 if $y_i &#x3D; c$ and 0 otherwise.</li>
<li>The class with the most occurrences is chosen.</li>
</ul>
<h2 id="Pros-and-Cons-of-KNN"><a href="#Pros-and-Cons-of-KNN" class="headerlink" title="Pros and Cons of KNN"></a>Pros and Cons of KNN</h2><p><strong>Advantages</strong>:</p>
<ul>
<li>Simple and easy to implement.</li>
<li>Effective for non-linear decision boundaries.</li>
<li>No need for training (lazy learning).</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Computationally expensive (slow for large datasets).</li>
<li>Sensitive to noise and irrelevant features.</li>
<li>Suffers from the <strong>curse of dimensionality</strong>.</li>
</ul>
<h2 id="Applications-of-KNN"><a href="#Applications-of-KNN" class="headerlink" title="Applications of KNN"></a>Applications of KNN</h2><ul>
<li><strong>Text classification</strong> (e.g., spam detection)</li>
<li><strong>Recommendation systems</strong> (e.g., movie or product recommendations)</li>
<li><strong>Medical diagnosis</strong> (e.g., predicting diseases based on similar cases)</li>
</ul>
<hr>
<h1 id="K-means-Clustering"><a href="#K-means-Clustering" class="headerlink" title="K-means Clustering"></a>K-means Clustering</h1><p>K-means is an <strong>unsupervised learning</strong> algorithm used for <strong>clustering</strong>.  </p>
<p>The core idea of K-means is:</p>
<blockquote>
<p>Partition the dataset into <strong>K clusters</strong> such that data points in the same cluster are similar to each other.</p>
</blockquote>
<p>It is an <strong>iterative optimization algorithm</strong> that minimizes intra-cluster distances.</p>
<p><strong>How K-means Works</strong>:</p>
<ol>
<li><strong>Initialize</strong> K cluster centroids (randomly selected).</li>
<li><strong>Assign each data point</strong> to the nearest centroid.</li>
<li><strong>Update centroids</strong> by computing the mean of all points in each cluster.</li>
<li>Repeat <strong>until centroids no longer change</strong> or a stopping criterion is met.</li>
</ol>
<h2 id="Mathematical-Formulation-1"><a href="#Mathematical-Formulation-1" class="headerlink" title="Mathematical Formulation"></a>Mathematical Formulation</h2><p>Objective Function (Loss Function):</p>
<p>K-means minimizes the sum of squared distances between points and their assigned cluster centers:</p>
<p>$$<br>J &#x3D; \sum_{i&#x3D;1}^{K} \sum_{x_j \in C_i} ||x_j - \mu_i||^2<br>$$</p>
<p>where:</p>
<ul>
<li>$K$ &#x3D; number of clusters</li>
<li>$C_i$ &#x3D; the $i$-th cluster</li>
<li>$x_j$ &#x3D; a data point in cluster $C_i$</li>
<li>$\mu_i$ &#x3D; centroid of cluster $C_i$</li>
</ul>
<p><strong>Updating Cluster Centers</strong>: The new centroid $\mu_i$ is computed as:</p>
<p>$$<br>\mu_i &#x3D; \frac{1}{|C_i|} \sum_{x_j \in C_i} x_j<br>$$</p>
<p>i.e., the mean of all points in the cluster.</p>
<h2 id="Pros-and-Cons-of-K-means"><a href="#Pros-and-Cons-of-K-means" class="headerlink" title="Pros and Cons of K-means"></a>Pros and Cons of K-means</h2><p><strong>Advantages</strong>:</p>
<ul>
<li>Simple and computationally efficient.</li>
<li>Works well on large datasets.</li>
<li>Produces interpretable results.</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Requires manually setting <strong>K</strong>.</li>
<li>Sensitive to <strong>initialization</strong> (may converge to local optima).</li>
<li>Struggles with non-convex cluster shapes.</li>
</ul>
<h2 id="Applications-of-K-means"><a href="#Applications-of-K-means" class="headerlink" title="Applications of K-means"></a>Applications of K-means</h2><ul>
<li><strong>Customer segmentation</strong> (e.g., marketing analytics)</li>
<li><strong>Image segmentation</strong> (e.g., clustering colors in an image)</li>
<li><strong>Anomaly detection</strong> (e.g., identifying outliers)</li>
</ul>
<hr>
<h1 id="KNN-vs-K-means-Key-Differences"><a href="#KNN-vs-K-means-Key-Differences" class="headerlink" title="KNN vs. K-means: Key Differences"></a>KNN vs. K-means: Key Differences</h1><table>
<thead>
<tr>
<th>Feature</th>
<th>KNN (K-Nearest Neighbors)</th>
<th>K-means Clustering</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Type</strong></td>
<td>Supervised learning</td>
<td>Unsupervised learning</td>
</tr>
<tr>
<td><strong>Purpose</strong></td>
<td>Classification &amp; Regression</td>
<td>Clustering</td>
</tr>
<tr>
<td><strong>Training</strong></td>
<td>No training required (lazy learning)</td>
<td>Requires iterative training</td>
</tr>
<tr>
<td><strong>Prediction</strong></td>
<td>Based on K nearest neighbors</td>
<td>Based on cluster centroids</td>
</tr>
<tr>
<td><strong>Distance metric</strong></td>
<td>Used to find nearest neighbors</td>
<td>Used to compute cluster assignments</td>
</tr>
<tr>
<td><strong>Computation cost</strong></td>
<td>High for large datasets</td>
<td>Lower after convergence</td>
</tr>
<tr>
<td><strong>Applications</strong></td>
<td>Spam detection, recommendation systems</td>
<td>Market segmentation, image compression</td>
</tr>
</tbody></table>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-02T15:50:57.000Z" title="2/2/2025, 3:50:57 PM">2025-02-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-02-16T02:45:33.363Z" title="2/16/2025, 2:45:33 AM">2025-02-16</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/02/Machine-Learning-5/">About Machine Learning ( Part 5: Support Vector Machine )</a></p><div class="content"><h1 id="Support-Vector-Machine-SVM"><a href="#Support-Vector-Machine-SVM" class="headerlink" title="Support Vector Machine (SVM)"></a>Support Vector Machine (SVM)</h1><p>Support Vector Machines (SVM) are one of the most powerful supervised learning algorithms used for classification and regression tasks.</p>
<h2 id="The-Hyperplane"><a href="#The-Hyperplane" class="headerlink" title="The Hyperplane"></a>The Hyperplane</h2><p>In a binary classification problem, the goal of SVM is to find a <strong>hyperplane</strong> that best separates two classes. Given a training dataset:</p>
<p>$$<br>D &#x3D; { (\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \dots, (\mathbf{x}_n, y_n) }, \quad \mathbf{x}_i \in \mathbb{R}^d, \quad y_i \in {-1, +1}<br>$$</p>
<ul>
<li>$\mathbf{x}_i$: $d$-dimensional feature vector (e.g., pixel values in an image).</li>
<li>$y_i$: Class label ($+1$ for “cat”, $-1$ for “dog”).</li>
</ul>
<h2 id="The-Hyperplane-1"><a href="#The-Hyperplane-1" class="headerlink" title="The Hyperplane"></a>The Hyperplane</h2><p>A hyperplane in $\mathbb{R}^d$ is defined as:</p>
<p>$$<br>\mathbf{w} \cdot \mathbf{x} + b &#x3D; 0, \quad \mathbf{x} \in \mathbb{R}^d<br>$$</p>
<ul>
<li>$\mathbf{w}$: <strong>Weight vector</strong> (normal to the hyperplane).</li>
<li>$b$: <strong>Bias term</strong> (shifts the hyperplane away from the origin).</li>
<li>$\mathbf{x}$: Any point in the feature space.</li>
</ul>
<h3 id="Example-2D-Hyperplane"><a href="#Example-2D-Hyperplane" class="headerlink" title="Example: 2D Hyperplane"></a>Example: 2D Hyperplane</h3><p>Consider a 2D hyperplane $2x_1 + 3x_2 - 12 &#x3D; 0$:</p>
<ul>
<li><strong>Normal Vector</strong>: $\mathbf{w} &#x3D; [2, 3]$.</li>
<li><strong>Bias</strong>: $b &#x3D; -12$.</li>
</ul>
<p><em>The weight vector $\mathbf{w}$ is perpendicular to the hyperplane.</em></p>
<p><img src="/blog/./img/ML5-2.png" alt="2x+3y-12=0"></p>
<h2 id="Margin-Maximization"><a href="#Margin-Maximization" class="headerlink" title="Margin Maximization"></a>Margin Maximization</h2><p>The distance from a sample $\mathbf{x}_i$ to the hyperplane is:</p>
<p>$$<br>\text{Distance} &#x3D; \frac{|\mathbf{w} \cdot \mathbf{x}_i + b|}{|\mathbf{w}|}.<br>$$</p>
<p>SVM seeks the hyperplane that <strong>maximizes the minimum margin</strong> between classes:</p>
<p>$$<br>\max_{\mathbf{w}, b} ( \frac{2}{|\mathbf{w}|} ) \quad \text{subject to} \quad y_i(\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1, , \forall i.<br>$$</p>
<p>This is equivalent to minimizing $|\mathbf{w}|^2$, a convex optimization problem solvable via quadratic programming.</p>
<h1 id="Optimize"><a href="#Optimize" class="headerlink" title="Optimize"></a>Optimize</h1><h2 id="Linear-Separation-with-Hard-Margin"><a href="#Linear-Separation-with-Hard-Margin" class="headerlink" title="Linear Separation with Hard Margin"></a>Linear Separation with Hard Margin</h2><p>When data is perfectly linearly separable, SVM seeks the hyperplane with maximum margin:</p>
<p>$$<br>\min_{\mathbf{w}, b} \frac{1}{2}|\mathbf{w}|^2 \quad \text{subject to} \quad y_i(\mathbf{w}^T\mathbf{x}_i + b) \geq 1, , \forall i<br>$$</p>
<p><strong>Geometric Interpretation</strong>:<br>The margin width is $\frac{2}{|\mathbf{w}|}$. Maximizing margin &#x3D; minimizing $|\mathbf{w}|$.</p>
<p><strong>The Limitation</strong>:</p>
<p>Fails catastrophically when:</p>
<ul>
<li>Data has noise&#x2F;outliers</li>
<li>Classes are inherently non-separable</li>
</ul>
<h2 id="Soft-Margin-SVM"><a href="#Soft-Margin-SVM" class="headerlink" title="Soft Margin SVM"></a>Soft Margin SVM</h2><p>Allow controlled violations using slack variables $\xi_i$:</p>
<p>$$<br>\begin{aligned}<br>\min_{\mathbf{w}, b, \xi} &amp;\quad \frac{1}{2}|\mathbf{w}|^2 + C\sum_{i&#x3D;1}^n \xi_i \<br>\text{s.t.} &amp;\quad y_i(\mathbf{w}^T\mathbf{x}_i + b) \geq 1 - \xi_i \<br>&amp;\quad \xi_i \geq 0, \quad \forall i<br>\end{aligned}<br>$$</p>
<ul>
<li><strong>$C$</strong>: Penalty weight (Large $C$ ≈ Hard Margin)</li>
<li><strong>$\xi_i$</strong>: How much the $i$-th sample violates the margin</li>
</ul>
<h3 id="Lagrangian-Formulation"><a href="#Lagrangian-Formulation" class="headerlink" title="Lagrangian Formulation"></a>Lagrangian Formulation</h3><p>Convert constraints into the objective function:</p>

$$
\mathcal{L} = \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_{i=1}^n \xi_i - \sum_{i=1}^n \alpha_i[y_i(\mathbf{w}^T\mathbf{x}_i + b) - 1 + \xi_i] - \sum_{i=1}^n \mu_i\xi_i
$$


<h4 id="Key-Derivations"><a href="#Key-Derivations" class="headerlink" title="Key Derivations:"></a>Key Derivations:</h4><ol>
<li><p><strong>Primal Variables</strong></p>
<ul>
<li>$\frac{\partial \mathcal{L}}{\partial \mathbf{w}} &#x3D; 0 \Rightarrow \mathbf{w} &#x3D; \sum \alpha_i y_i \mathbf{x}_i$</li>
<li>$\frac{\partial \mathcal{L}}{\partial b} &#x3D; 0 \Rightarrow \sum \alpha_i y_i &#x3D; 0$</li>
<li>$\frac{\partial \mathcal{L}}{\partial \xi_i} &#x3D; 0 \Rightarrow \alpha_i + \mu_i &#x3D; C$</li>
</ul>
</li>
<li><p><strong>Dual Problem</strong><br>Substitute back to get:</p>
</li>
</ol>
<p>$$<br>\max_{\alpha} \sum_{i&#x3D;1}^n \alpha_i - \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j \<br>\text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \sum \alpha_i y_i &#x3D; 0<br>$$</p>
<h3 id="Interpretation-of-alpha-i"><a href="#Interpretation-of-alpha-i" class="headerlink" title="Interpretation of $\alpha_i$"></a>Interpretation of $\alpha_i$</h3><table>
<thead>
<tr>
<th>$\alpha_i$ Range</th>
<th>Sample Status</th>
<th>$\xi_i$ Value</th>
</tr>
</thead>
<tbody><tr>
<td>$&#x3D;0$</td>
<td>Outside margin</td>
<td>0</td>
</tr>
<tr>
<td>$(0, C)$</td>
<td>On margin</td>
<td>0</td>
</tr>
<tr>
<td>$&#x3D;C$</td>
<td>Inside margin</td>
<td>$&gt;0$</td>
</tr>
</tbody></table>
<p><strong>Decision Function</strong>:</p>
<p>$$<br>f(\mathbf{x}) &#x3D; \text{sign}( \sum_{\alpha_i &gt; 0} \alpha_i y_i \mathbf{x}_i^T \mathbf{x} + b)<br>$$</p>
<h2 id="Nonlinear-Classification-with-Kernel-Trick"><a href="#Nonlinear-Classification-with-Kernel-Trick" class="headerlink" title="Nonlinear Classification with Kernel Trick"></a>Nonlinear Classification with Kernel Trick</h2><h3 id="The-Fundamental-Idea"><a href="#The-Fundamental-Idea" class="headerlink" title="The Fundamental Idea"></a>The Fundamental Idea</h3><p><strong>Problem</strong>: Many datasets require nonlinear boundaries.<br><strong>Solution</strong>: Map data to higher dimension $\phi(\mathbf{x})$ where linear separation becomes possible.</p>
<p><strong>Example Transformation</strong>:<br>For $\mathbf{x} &#x3D; [x_1, x_2]$, use $\phi(\mathbf{x}) &#x3D; [x_1, x_2, x_1^2 + x_2^2]$</p>
<h3 id="The-Computational-Challenge"><a href="#The-Computational-Challenge" class="headerlink" title="The Computational Challenge"></a>The Computational Challenge</h3><p>Direct computation of $\phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)$ in high dimensions is intractable.</p>
<p><strong>Key Insight</strong>: Many ML algorithms (like SVM) only need inner products, not explicit coordinates.</p>
<h3 id="Kernel-Functions-to-the-Rescue"><a href="#Kernel-Functions-to-the-Rescue" class="headerlink" title="Kernel Functions to the Rescue"></a>Kernel Functions to the Rescue</h3><p>Replace $\phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)$ with kernel $K(\mathbf{x}_i, \mathbf{x}_j)$:</p>
<p><strong>Updated Dual Problem</strong>:</p>
<p>$$<br>\max_{\alpha} \sum_{i&#x3D;1}^n \alpha_i - \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j K(\mathbf{x}_i, \mathbf{x}_j)<br>$$</p>
<h4 id="Common-Kernels"><a href="#Common-Kernels" class="headerlink" title="Common Kernels:"></a>Common Kernels:</h4><table>
<thead>
<tr>
<th>Kernel</th>
<th>Formula</th>
<th>Characteristics</th>
</tr>
</thead>
<tbody><tr>
<td>Linear</td>
<td>$K(\mathbf{x}, \mathbf{z}) &#x3D; \mathbf{x}^T\mathbf{z}$</td>
<td>No transformation</td>
</tr>
<tr>
<td>Polynomial</td>
<td>$(\mathbf{x}^T\mathbf{z} + c)^d$</td>
<td>Captures polynomial interactions</td>
</tr>
<tr>
<td>RBF</td>
<td>$\exp(-\gamma |\mathbf{x}-\mathbf{z}|^2)$</td>
<td>Infinite-dimensional mapping</td>
</tr>
<tr>
<td>Sigmoid</td>
<td>$\tanh(\alpha \mathbf{x}^T\mathbf{z} + c)$</td>
<td>Mimics neural networks</td>
</tr>
</tbody></table>
<h3 id="3-4-Why-Kernels-Work-Mercer’s-Theorem"><a href="#3-4-Why-Kernels-Work-Mercer’s-Theorem" class="headerlink" title="3.4 Why Kernels Work: Mercer’s Theorem"></a>3.4 Why Kernels Work: Mercer’s Theorem</h3><p>A valid kernel must:</p>
<ol>
<li>Be symmetric: $K(\mathbf{x}, \mathbf{z}) &#x3D; K(\mathbf{z}, \mathbf{x})$</li>
<li>Produce positive semi-definite Gram matrix</li>
</ol>
<p><strong>Practical Check</strong>: If SVM training converges, your kernel is valid.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-01-21T15:50:57.000Z" title="1/21/2025, 3:50:57 PM">2025-01-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-02-16T02:45:33.363Z" title="2/16/2025, 2:45:33 AM">2025-02-16</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/01/21/Machine-Learning-4/">About Machine Learning ( Part 4: Decision Tree )</a></p><div class="content"><p>A Decision Tree is a supervised learning algorithm used for both classification and regression tasks. It organizes data into a tree-like structure, where each internal node represents a decision based on a feature, and each leaf node provides a prediction. Decision trees are simple, interpretable, and capable of handling both categorical and numerical data.</p>
<h1 id="Classification-Tree"><a href="#Classification-Tree" class="headerlink" title="Classification Tree"></a>Classification Tree</h1><p>A <strong>Classification Tree</strong> is a decision tree used for classifying data into distinct categories or classes. The main objective of a classification tree is to predict the category or class to which a given input belongs based on various features.</p>
<p><img src="/blog/./img/ML4-1.png"></p>
<h2 id="How-Does-a-Classification-Tree-Work"><a href="#How-Does-a-Classification-Tree-Work" class="headerlink" title="How Does a Classification Tree Work?"></a>How Does a Classification Tree Work?</h2><p>The tree is constructed in the following steps:</p>
<ol>
<li><strong>Select the best feature</strong>: The first step is to choose the feature that best splits the data. This is usually done by calculating the <strong>Entropy</strong>, such as Information Gain, or Gini Index.</li>
<li><strong>Split the data</strong>: The chosen feature is used to split the dataset into two or more subsets. The splitting process continues recursively until the stopping criteria are met (e.g., maximum depth reached or all data in a node belong to the same class).</li>
<li><strong>Predict the class</strong>: The leaf nodes represent the predicted class labels, which are determined based on the majority class in that node.</li>
</ol>
<h2 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h2><p><strong>Entropy</strong> measures the disorder or uncertainty in a dataset. It quantifies the impurity of a dataset. If the dataset is perfectly pure (i.e., all samples belong to the same class), <strong>the entropy is 0</strong>. If the dataset has an equal distribution of all possible classes, the entropy reaches its maximum value.</p>
<p>The formula for entropy $H(S)$ of a set $S$ is:</p>
<p>$$<br>H(S) &#x3D; - \sum_{x \in S} p(x) \log_2 p(x)<br>$$</p>
<p>Where:</p>
<ul>
<li>$S$ is the dataset.</li>
<li>$p(x)$ is the proportion of instances in $S$ that belong to class $x$.</li>
</ul>
<p>For binary classification (i.e., two classes, say “Yes” and “No”), the entropy simplifies to:</p>
<p>$$<br>H(S) &#x3D; - p(+) \log_2 p(+) - p(-) \log_2 p(-)<br>$$</p>
<p>Where:</p>
<ul>
<li>$p(+)$ is the proportion of “Yes” labels in the dataset.</li>
<li>$p(-)$ is the proportion of “No” labels in the dataset.</li>
</ul>
<p>Interpretation of Entropy:</p>
<ul>
<li>If the entire dataset belongs to one class (e.g., all “Yes” or all “No”), then the entropy is 0 because there is no uncertainty.</li>
<li>If the dataset has an equal distribution of both classes (e.g., $p(+) &#x3D; p(-) &#x3D; 0.5$), the entropy is 1 because there is maximum uncertainty.</li>
</ul>
<p>For example, in the case of a <strong>Tennis Playing</strong> dataset, if the target variable (whether a person will play tennis or not) is evenly split between “Yes” and “No”, the entropy will be:</p>
<p>$$<br>H(S) &#x3D; - 0.5 \log_2 0.5 - 0.5 \log_2 0.5 &#x3D; 1<br>$$</p>
<p>This means the data is maximally <strong>uncertain</strong> (a 50&#x2F;50 chance of playing or not playing).</p>
<p><img src="/blog/./img/ML4-2.png" alt="https://en.wikipedia.org/wiki/Binary_entropy_function"></p>
<h2 id="Information-Gain"><a href="#Information-Gain" class="headerlink" title="Information Gain"></a>Information Gain</h2><p><strong>Information Gain (IG)</strong> measures how well an attribute (or feature) separates the dataset into distinct classes. It is based on the difference in <strong>entropy</strong> before and after the split. The goal is to reduce uncertainty or disorder in the data as much as possible by selecting the attribute.</p>
<p>The formula for <strong>Information Gain</strong> when splitting a dataset $S$ based on an attribute $A$ is:</p>
<p>$$<br>IG(S, A) &#x3D; H(S) - \sum_{v \in V(A)} \frac{|S_v|}{|S|} H(S_v)<br>$$</p>
<p>Where:</p>
<ul>
<li>$H(S)$ is the entropy of the dataset $S$ before the split.</li>
<li>$H(S_v)$ is the entropy of the subset $S_v$.</li>
<li>$V(A)$ is the set of all possible values for attribute $A$.</li>
<li>$\frac{|S_v|}{|S|}$ is the proportion of the data in subset $S_v$ relative to the entire dataset $S$. ( Weighted )</li>
</ul>
<p>The higher the information gain, the better the attribute is at reducing uncertainty and distinguishing between different classes. A high information gain indicates that the attribute is good at separating the data into pure subsets.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/kongchenglc/Machine-Learning-Examples/blob/master/decision-tree1.py">Code Demo</a></p>
<h2 id="Gini-Index"><a href="#Gini-Index" class="headerlink" title="Gini Index"></a>Gini Index</h2><p><strong>Gini Index</strong> is another measure used to evaluate the quality of a split in a decision tree. It measures the impurity or disorder of a dataset, with a lower Gini index indicating a purer dataset.</p>
<p>The <strong>Gini index</strong> for a dataset ( S ) is calculated as:</p>
<p>$$<br>Gini(S) &#x3D; 1 - \sum_{i&#x3D;1}^{C} p_i^2<br>$$</p>
<p>Where:</p>
<ul>
<li>( C ) is the number of classes in the target variable.</li>
<li>( p_i ) is the proportion of the samples in the dataset ( S ) that belong to class ( i ).</li>
</ul>
<p>Interpretation:</p>
<ul>
<li>If all the data points belong to a single class, the Gini index is 0, indicating a pure node.</li>
<li>If the data points are evenly distributed among all classes, the Gini index is maximized (impure node). For binary classification, the maximum value is 0.5.</li>
</ul>
<h2 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h2><p><strong>Overfitting</strong> occurs when a decision tree becomes too complex, capturing noise in the data instead of general patterns. It performs well on training data but poorly on new data.</p>
<blockquote>
<p>Given a hypothesis space $H$, a hypothesis $h \in H$ is said to overfit the training data if there exists some alternative hypothesis $h’ \in H$, such that $h$ has a smaller error than $h’$ over the training examples, but $h’$ has a smaller error than $h$ over the entire distribution of instances.<br>– Tom Mitchell</p>
</blockquote>
<p><strong>Causes:</strong> Noisy data or insufficient data can lead to overfitting.</p>
<p><strong>Solutions</strong>:</p>
<ol>
<li><strong>Stop growing</strong> the tree once it reaches a certain depth.</li>
<li><strong>Prune</strong> the tree by removing branches with little importance.</li>
</ol>
<h2 id="Pruning"><a href="#Pruning" class="headerlink" title="Pruning"></a>Pruning</h2><p>Pruning reduces tree complexity by removing unnecessary nodes.</p>
<p><strong>Process</strong>:</p>
<ol>
<li>Turn a node into a leaf with the most common value in that subset.</li>
<li>Test the accuracy of the pruned tree.</li>
<li>If accuracy improves, keep the change; if not, restore the node.</li>
</ol>
<p><strong>Data Splits</strong>:</p>
<ul>
<li><strong>Training set</strong> for building the tree.</li>
<li><strong>Testing set</strong> for evaluating performance.</li>
<li><strong>Pruning set</strong> for pruning decisions.</li>
</ul>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Simplifies the model.</li>
<li>Reduces overfitting and improves generalization.</li>
<li>Results in better test data performance.</li>
</ul>
<h1 id="Regression-Tree"><a href="#Regression-Tree" class="headerlink" title="Regression Tree"></a>Regression Tree</h1><p>A <strong>Regression Tree</strong> is a type of decision tree used for predicting continuous target variables. Unlike classification trees that predict discrete labels, regression trees predict numerical values. Here’s how the process works:</p>
<ol>
<li><p><strong>Choosing Features and Split Values</strong></p>
<ul>
<li>The goal is to find the best feature and threshold to split the data. This is done by minimizing the variance or mean squared error (MSE) within each subset after the split.</li>
<li>For example, given a feature $X$ and target $Y$, we want to find a threshold $t$ to split the data into two subsets: $X \leq t$ and $X &gt; t$. The split that minimizes the variance within each subset is chosen.</li>
</ul>
</li>
<li><p><strong>Splitting the Data</strong></p>
<ul>
<li>Once the best feature and threshold are identified, the data is split into two subsets based on this threshold.</li>
<li>This is a recursive process, where each subset is further split until a stopping condition (e.g., maximum depth or minimum sample size) is met.</li>
</ul>
</li>
<li><p><strong>Recursive Splitting</strong></p>
<ul>
<li>At each node, the algorithm continues splitting based on the feature that minimizes the variance.</li>
<li>For example, a tree might first split the data based on $X &#x3D; 5$, then further split the subset $X &gt; 5$ based on $X &#x3D; 7$.</li>
</ul>
</li>
<li><p><strong>Leaf Nodes and Predictions</strong></p>
<ul>
<li>When the tree reaches the stopping condition, each leaf node represents a final prediction, which is the mean target value of the data points in that node.</li>
<li>For instance, if a leaf node contains values $Y &#x3D; 10, 12, 14$, the prediction for that leaf would be the average $12$.</li>
</ul>
</li>
<li><p><strong>Prediction for New Data</strong></p>
<ul>
<li>For new data, the tree traverses from the root to a leaf node, where the predicted value is the mean of the target values in that leaf.</li>
</ul>
</li>
</ol>
<h1 id="Bagging-vs-Boosting"><a href="#Bagging-vs-Boosting" class="headerlink" title="Bagging vs Boosting"></a>Bagging vs Boosting</h1><p>In machine learning, <strong>Bagging</strong> and <strong>Boosting</strong> are two popular ensemble techniques used to improve the performance of models. These methods <strong>combine the predictions of multiple base learners to form a stronger model</strong>, but they approach this goal in different ways.</p>
<h2 id="Bagging-Bootstrap-Aggregating"><a href="#Bagging-Bootstrap-Aggregating" class="headerlink" title="Bagging (Bootstrap Aggregating)"></a>Bagging (Bootstrap Aggregating)</h2><p><strong>Bagging</strong> is an ensemble technique that reduces variance by training multiple base learners independently on different subsets of the data and then combining their predictions.</p>
<h3 id="1-How-Bagging-Works"><a href="#1-How-Bagging-Works" class="headerlink" title="1. How Bagging Works"></a>1. <strong>How Bagging Works</strong></h3><ol>
<li><strong>Data Sampling</strong>: Multiple subsets of the training data are created by random sampling with replacement (bootstrap sampling).</li>
<li><strong>Train Multiple Models</strong>: Each subset is used to train a separate model, typically the same type of model (e.g., decision trees).</li>
<li><strong>Combine Predictions</strong>: For classification problems, the final prediction is the class that receives the most votes from all base learners (majority voting). For regression problems, the final prediction is the average of all base learner predictions.</li>
</ol>
<h3 id="2-Advantages"><a href="#2-Advantages" class="headerlink" title="2. Advantages"></a>2. <strong>Advantages</strong></h3><ul>
<li>Reduces overfitting, particularly with high-variance models (e.g., decision trees).</li>
<li>Works well with unstable base learners.</li>
</ul>
<h3 id="3-Disadvantages"><a href="#3-Disadvantages" class="headerlink" title="3. Disadvantages"></a>3. <strong>Disadvantages</strong></h3><ul>
<li>Doesn’t perform as well when base learners are weak or the model complexity is too high.</li>
</ul>
<h3 id="4-Example-Algorithm"><a href="#4-Example-Algorithm" class="headerlink" title="4. Example Algorithm"></a>4. <strong>Example Algorithm</strong></h3><ul>
<li><strong>Random Forest</strong>: A popular implementation of Bagging, where base learners are decision trees trained on random subsets of features.</li>
</ul>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p><strong>Boosting</strong> is an ensemble technique that improves weak learners by iteratively training them on the data and adjusting the weights to focus on the mistakes of previous models.</p>
<h3 id="1-How-Boosting-Works"><a href="#1-How-Boosting-Works" class="headerlink" title="1. How Boosting Works"></a>1. <strong>How Boosting Works</strong></h3><ol>
<li><strong>Train the First Model</strong>: Train a base learner on the training data.</li>
<li><strong>Calculate Error</strong>: Calculate the errors made by the first model.</li>
<li><strong>Adjust Weights</strong>: Increase the weights of the misclassified data points, so that the next model will focus more on them.</li>
<li><strong>Train Subsequent Models</strong>: Train the next model using the updated weights.</li>
<li><strong>Combine Predictions</strong>: The final prediction is a weighted average (regression) or a weighted vote (classification) of all base learners.</li>
</ol>
<h3 id="2-Advantages-1"><a href="#2-Advantages-1" class="headerlink" title="2. Advantages"></a>2. <strong>Advantages</strong></h3><ul>
<li>Reduces bias, improving model accuracy by focusing on hard-to-predict examples.</li>
<li>Works well with weak learners that have high bias.</li>
</ul>
<h3 id="3-Disadvantages-1"><a href="#3-Disadvantages-1" class="headerlink" title="3. Disadvantages"></a>3. <strong>Disadvantages</strong></h3><ul>
<li>Can overfit if data is noisy or contains outliers.</li>
<li>Computationally expensive since models are trained sequentially.</li>
</ul>
<h3 id="4-Example-Algorithms"><a href="#4-Example-Algorithms" class="headerlink" title="4. Example Algorithms"></a>4. <strong>Example Algorithms</strong></h3><ul>
<li><strong>AdaBoost</strong>: A widely used Boosting algorithm that adjusts sample weights based on misclassifications.</li>
<li><strong>Gradient Boosting</strong>: An improved version of Boosting that optimizes the model using gradient descent.</li>
<li><strong>XGBoost</strong>: A highly efficient implementation of Gradient Boosting, popular in machine learning competitions.</li>
</ul>
<p><img src="/blog/./img/ML4-3.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-01-16T18:50:05.000Z" title="1/16/2025, 6:50:05 PM">2025-01-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-02-16T02:45:33.363Z" title="2/16/2025, 2:45:33 AM">2025-02-16</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/01/16/Machine-Learning-3/">About Machine Learning ( Part 3: Logistic Regression )</a></p><div class="content"><h2 id="Classification-Problem"><a href="#Classification-Problem" class="headerlink" title="Classification Problem"></a>Classification Problem</h2><p>In machine learning, when we are predicting a discrete label, such as determining whether an email is spam or not, we are dealing with a classification problem. Logistic regression is commonly used for <strong>binary classification</strong> tasks, where the goal is to predict one of two classes, typically represented as 0 or 1.</p>
<p>The logistic function (also called the <strong>sigmoid function</strong>) is the core of logistic regression, as it maps input features to probabilities between 0 and 1. These probabilities represent the likelihood of the sample belonging to a particular class.</p>
<p>The logistic function is defined as:</p>
<p>$$<br>\sigma(z) &#x3D; \frac{1}{1 + e^{-z}}<br>$$</p>
<p>Where $z &#x3D; \omega_0 + \mathbf{\omega}^T \mathbf{x}$, the linear combination of the input features $\mathbf{x}$ and the model’s parameters $\mathbf{\omega}$.</p>
<h2 id="Model-Representation"><a href="#Model-Representation" class="headerlink" title="Model Representation"></a>Model Representation</h2><p>In logistic regression, we aim to predict the probability that an observation $\mathbf{x}$ belongs to class 1. The predicted probability is given by the following equation:</p>
<p>$$<br>p(C &#x3D; 1|\mathbf{x}) &#x3D; \sigma(\omega_0 + \mathbf{\omega}^T \mathbf{x}) &#x3D; \frac{1}{1 + e^{-(\omega_0 + \mathbf{\omega}^T \mathbf{x})}}<br>$$</p>
<p>where:</p>
<ul>
<li>$p(C &#x3D; 1|\mathbf{x})$: The probability that the class label $C$ is 1, given the input features $\mathbf{x}$.</li>
<li>$\omega_0$: The <strong>bias</strong> term, which helps adjust the output independently of the input features.</li>
<li>$\mathbf{\omega}$: The <strong>weight vector</strong> that contains the coefficients for each feature.</li>
<li>$\mathbf{\omega}^T \mathbf{x}$: The <strong>dot product</strong> between the weight vector $\mathbf{\omega}$ and the feature vector $\mathbf{x}$, representing the weighted sum of the input features.</li>
</ul>
<h2 id="Decision-Boundary"><a href="#Decision-Boundary" class="headerlink" title="Decision Boundary"></a>Decision Boundary</h2><p>In binary classification, the <strong>decision boundary</strong> is the point where the model predicts equal probabilities for both classes, meaning the probability of being in class 0 is 0.5 and the probability of being in class 1 is also 0.5. This boundary helps separate the two classes.</p>
<p>We calculate this boundary by setting the predicted probability equal to 0.5:</p>
<p>$$<br>p(C &#x3D; 1|\mathbf{x}) &#x3D; 0.5<br>$$</p>
<p><img src="/blog/./img/ML3-1.png"></p>
<p>This happens when the output of the logistic function equals 0.5. Solving for the decision boundary, we get:</p>
<p>$$<br>\sigma(\omega_0 + \mathbf{\omega}^T \mathbf{x}) &#x3D; 0.5<br>$$</p>
<p>This implies:</p>
<p>$$<br>\omega_0 + \mathbf{\omega}^T \mathbf{x} &#x3D; 0<br>$$</p>
<p>This equation represents the decision boundary where the model will predict a 50% chance of the sample belonging to either class. Points on this boundary are classified as uncertain.</p>
<h2 id="Maximum-Likelihood-Estimation-MLE"><a href="#Maximum-Likelihood-Estimation-MLE" class="headerlink" title="Maximum Likelihood Estimation (MLE)"></a>Maximum Likelihood Estimation (MLE)</h2><p>In logistic regression, the goal is to find the parameters $\mathbf{\omega} &#x3D; (\omega_0, \omega_1, …, \omega_d)$ that maximize the likelihood of observing the training data. The likelihood function $L(\mathbf{\omega})$ is the probability of the observed labels given the feature vectors.</p>
<p>If We assume that the data is independent and<br>identically distributed (IDD). The likelihood function will be:</p>
<p>$$<br>L(\mathbf{\omega}) &#x3D; \prod_{i&#x3D;1}^{N} p(t_i | \mathbf{x}_i; \mathbf{\omega})<br>$$</p>
<p>Where:</p>
<ul>
<li>$N$: The number of training samples.</li>
<li>$t_i$: The actual label for the $i$-th sample.</li>
<li>$\mathbf{x}_i$: The feature vector for the $i$-th sample.</li>
<li>$p(t_i | \mathbf{x}_i; \mathbf{\omega})$: The probability of observing label $t_i$ given the features $\mathbf{x}_i$ and parameters $\mathbf{\omega}$.</li>
</ul>
<p>Maximizing this likelihood function helps us find the optimal values for the model’s parameters.</p>
<h2 id="Log-Likelihood-Function"><a href="#Log-Likelihood-Function" class="headerlink" title="Log-Likelihood Function"></a>Log-Likelihood Function</h2><p>However, when we have many samples ($N$ is large) and each probability $p(t_i | \mathbf{x}_i; \mathbf{\omega})$ is a value less than 1, the product of these probabilities becomes very small. This leads to a problem called <strong>Numerical Underflow</strong>, where the computer cannot handle such small numbers.</p>
<p>So, maximizing the likelihood function directly is difficult due to the product of probabilities. Instead, we take the <strong>log-likelihood</strong>, which simplifies the optimization by turning the product into a sum:</p>
<p>$$<br>\ln(L(\mathbf{\omega})) &#x3D; \sum_{i&#x3D;1}^{N} \left[ t_i \ln(p(t_i &#x3D; 1|\mathbf{x}_i; \mathbf{\omega})) + (1 - t_i) \ln(1 - p(t_i &#x3D; 1|\mathbf{x}_i; \mathbf{\omega})) \right]<br>$$</p>
<p>This form assumes a <strong>binary classification</strong> scenario, where each label $t_i$ can only be either 0 or 1. In such a case:</p>
<ul>
<li>If $t_i &#x3D; 1$, we calculate the log of the predicted probability for class 1: $\ln(p(t_i &#x3D; 1|\mathbf{x}_i; \mathbf{\omega}))$.</li>
<li>If $t_i &#x3D; 0$, we calculate the log of the probability of class 0, which is $1 - p(t_i &#x3D; 1|\mathbf{x}_i; \mathbf{\omega})$: $\ln(1 - p(t_i &#x3D; 1|\mathbf{x}_i; \mathbf{\omega}))$.</li>
</ul>
<p>This reflects the basic assumption of binary classification in logistic regression, where the goal is to predict the probability of an input sample belonging to class 1 (or class 0, which is just the complement of class 1).</p>
<h2 id="Gradient-Descent-for-Optimization"><a href="#Gradient-Descent-for-Optimization" class="headerlink" title="Gradient Descent for Optimization"></a>Gradient Descent for Optimization</h2><p>We use <strong>gradient descent</strong> to maximize the log-likelihood function. Gradient descent involves computing the gradient of the log-likelihood with respect to the parameters $\mathbf{\omega}$, and then updating the parameters in the direction that increases the log-likelihood.</p>
<p>The gradient of the log-likelihood function with respect to each parameter $\omega_d$ is:</p>

$$
\frac{\partial \ln(L(\mathbf{\omega}))}{\partial \omega_d} = \sum_{i=1}^{N} ( t_i - p(C = 1 \mid \mathbf{x}_i; \mathbf{\omega})) x_{i,d}
$$


<p>Where:</p>
<ul>
<li>$x_{i,d}$ is the $d$-th feature of the $i$-th sample.</li>
<li>$p(C &#x3D; 1|\mathbf{x}_i; \mathbf{\omega})$ is the predicted probability of class 1 for the $i$-th sample.</li>
<li>$t_i$ is the <strong>true label</strong> of the $i$-th sample, where $t_i &#x3D; 1$ if the sample belongs to class 1 (positive class), and $t_i &#x3D; 0$ if it belongs to class 0 (negative class).</li>
</ul>
<p>Explanation:</p>
<ul>
<li>The term $(t_i - p(C &#x3D; 1|\mathbf{x}_i; \mathbf{\omega}))$ represents the <strong>error</strong> between the true label and the predicted probability for the $i$-th sample.</li>
<li>The product of this error and the corresponding feature $x_{i,d}$ allows us to adjust the weight $\omega_d$ based on how the feature $x_{i,d}$ contributes to the error.</li>
</ul>
<p>By summing over all $N$ samples, the gradient is computed for each parameter $\omega_d$. Using the gradient, we update the parameters $\mathbf{\omega}$ using the following rule:</p>
<p>$$<br>\omega_d \leftarrow \omega_d - \lambda \frac{\partial \ln(L(\mathbf{\omega}))}{\partial \omega_d}<br>$$</p>
<p>Where:</p>
<ul>
<li>$\lambda$ is the learning rate, which controls the step size during optimization.</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/kongchenglc/Machine-Learning-Examples/blob/master/logistic-regression3.py">Demo Code</a></p>
<h2 id="Confusion-Matrix"><a href="#Confusion-Matrix" class="headerlink" title="Confusion Matrix"></a>Confusion Matrix</h2><!-- ![](../img/ML3-2.png) -->

<p>The confusion matrix is a table that summarizes the performance of a classifier on a set of test data for which the true values are known.</p>
<table>
<thead>
<tr>
<th></th>
<th>Predicted Positive</th>
<th>Predicted Negative</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Actual Positive</strong></td>
<td>True Positive (TP)</td>
<td>False Negative (FN)</td>
</tr>
<tr>
<td><strong>Actual Negative</strong></td>
<td>False Positive (FP)</td>
<td>True Negative (TN)</td>
</tr>
</tbody></table>
<ul>
<li><strong>True Positive (TP):</strong> Correctly predicted positive instances.</li>
<li><strong>True Negative (TN):</strong> Correctly predicted negative instances.</li>
<li><strong>False Positive (FP):</strong> Negative instances incorrectly predicted as positive.</li>
<li><strong>False Negative (FN):</strong> Positive instances incorrectly predicted as negative.</li>
</ul>
<h3 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h3><p>Accuracy measures the proportion of correctly classified instances (both positive and negative) out of all predictions.</p>
<p><strong>Formula:</strong></p>
<p>$$<br>\text{Accuracy} &#x3D; \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}<br>$$</p>
<p><strong>When to use Accuracy:</strong></p>
<ul>
<li>When the dataset is balanced, meaning the number of positive and negative instances is roughly equal.</li>
</ul>
<hr>
<h3 id="Precision-Positive-Predictive-Value"><a href="#Precision-Positive-Predictive-Value" class="headerlink" title="Precision (Positive Predictive Value)"></a>Precision (Positive Predictive Value)</h3><p>Precision quantifies the proportion of positive predictions that are correct.</p>
<p><strong>Formula:</strong></p>
<p>$$<br>\text{Precision} &#x3D; \frac{\text{TP}}{\text{TP} + \text{FP}}<br>$$</p>
<p><strong>Use case for Precision:</strong></p>
<ul>
<li>When false positives have a high cost (e.g., flagging legitimate emails as spam).</li>
</ul>
<hr>
<h3 id="Recall-Sensitivity-or-True-Positive-Rate"><a href="#Recall-Sensitivity-or-True-Positive-Rate" class="headerlink" title="Recall (Sensitivity or True Positive Rate)"></a>Recall (Sensitivity or True Positive Rate)</h3><p>Recall measures the proportion of actual positives that are correctly identified.</p>
<p><strong>Formula:</strong></p>
<p>$$<br>\text{Recall} &#x3D; \frac{\text{TP}}{\text{TP} + \text{FN}}<br>$$</p>
<p><strong>Use case for Recall:</strong></p>
<ul>
<li>When false negatives have a high cost (e.g., failing to detect a disease in medical testing).</li>
</ul>
<hr>
<h3 id="F1-Score-Harmonic-Mean-of-Precision-and-Recall"><a href="#F1-Score-Harmonic-Mean-of-Precision-and-Recall" class="headerlink" title="F1-Score (Harmonic Mean of Precision and Recall)"></a>F1-Score (Harmonic Mean of Precision and Recall)</h3><p>The F1-Score combines Precision and Recall into a single metric, especially useful when you need to balance the trade-off between the two.</p>
<p><strong>Formula:</strong></p>
<p>$$<br>F_1 &#x3D; 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}<br>$$</p>
<p><strong>Why use F1-Score?</strong></p>
<ul>
<li>It is beneficial when dealing with imbalanced datasets, as it considers both false positives and false negatives.</li>
</ul>
<hr>
<h3 id="Key-Insights"><a href="#Key-Insights" class="headerlink" title="Key Insights"></a>Key Insights</h3><ul>
<li><strong>Accuracy</strong> works well on balanced datasets but may be misleading when classes are imbalanced.</li>
<li><strong>Precision</strong> is crucial when false positives are costly.</li>
<li><strong>Recall</strong> is critical when false negatives are costly.</li>
<li><strong>F1-Score</strong> provides a balanced measure when both Precision and Recall are important.</li>
</ul>
<p>By carefully analyzing the confusion matrix and the derived metrics, you can fine-tune your model for optimal performance, depending on the specific requirements of your application.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-01-07T16:41:21.000Z" title="1/7/2025, 4:41:21 PM">2025-01-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-02-16T02:45:33.362Z" title="2/16/2025, 2:45:33 AM">2025-02-16</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/01/07/Machine-Learning-2/">About Machine Learning ( Part 2: Linear Regression )</a></p><div class="content"><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>In prediction tasks, we often use independent features to predict a dependent variable. If we have a dataset:</p>
<p>$$<br>{ x_d^{(i)}, t^{(i)} }<br>$$</p>
<p>where:</p>
<ul>
<li>$x_d^{(i)}$: The $d$-th feature of the $i$-th instance in the dataset.</li>
<li>$t^{(i)}$: The target value (dependent variable) for the $i$-th instance.</li>
<li>$i &#x3D; 1, \dots, N$: $i$ indexes the instances, and $N$ is the total number of instances in the dataset. ( Here $i$ is not power )</li>
<li>$d &#x3D; 1, \dots, D$: $d$ indexes the features, and $D$ is the total number of independent features.</li>
</ul>
<p>Each feature in the dataset can be expressed as:</p>
<p>$$<br>x_d^{(i)}<br>$$</p>
<p>For simplicity, the following focuses on a <strong>single feature</strong> $x$, meaning $D &#x3D; 1$.</p>
<h2 id="Polynomial"><a href="#Polynomial" class="headerlink" title="Polynomial"></a>Polynomial</h2><p>This is a <strong>curve fitting</strong> problem, where we aim to fit a polynomial function to model the relationship between the independent variable $x$ and the dependent variable $t$.</p>
<p>A polynomial model is expressed as:</p>
<p>$$<br>h(x, \omega) &#x3D; \omega_0 + \omega_1 x + \omega_2 x^2 + \cdots + \omega_M x^M<br>$$</p>
<ul>
<li>$h(x, \omega)$: The predicted output (dependent variable) for a given input $x$.</li>
<li>$\omega_0, \omega_1, \dots, \omega_M$: The coefficients (parameters) of the polynomial.</li>
<li>$M$: The order of the polynomial, which represents the highest power of $x$ used in the model.</li>
</ul>
<p>This expanded form explicitly shows all terms of the polynomial up to order $M$.</p>
<p>It can also be written in a more compact form using summation:</p>
<p>$$<br>h(x, \omega) &#x3D; \omega_0 + \sum_{j&#x3D;1}^{M} \omega_j x^j<br>$$</p>
<p>Here:</p>
<ul>
<li>The summation $\sum_{j&#x3D;1}^{M} \omega_j x^j$ compactly represents all terms from $j &#x3D; 1$ (first-order) to $j &#x3D; M$ (highest-order).</li>
<li>$\omega_0$: The constant term (bias), which is excluded from the summation since it is independent of $x$.</li>
</ul>
<h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><p>For a straight line, the model is a linear function of the form:</p>
<p>$$<br>f(x, \omega) &#x3D; \omega_0 + \omega_1 x<br>$$</p>
<p>Where:</p>
<ul>
<li>$M &#x3D; 1$, indicating that the polynomial is of degree 1 (a straight line).</li>
<li>$f(x, \omega) &#x3D; \omega_0 + \omega_1 x$ is the equation of the line.</li>
<li>$\omega_1$ represents the slope of the line, and $\omega_0$ represents the y-intercept in the equation.</li>
</ul>
<p>We can use optimization to minimize the overall error. The cost function $J(\omega)$ is defined as:</p>
<p>$$<br>J(\omega) &#x3D; \frac{1}{2} \sum_{n&#x3D;1}^{N} \left(t_n - f(x_n, \omega)\right)^2<br>$$</p>
<p>Where:</p>
<ul>
<li>$J(\omega)$ is the cost function that we aim to minimize ( MSE ).</li>
<li>$t_n$ is the target value (actual value) for the $n$-th data point.</li>
<li>$f(x_n, \omega)$ is the predicted value from the model for the $n$-th data point.</li>
<li>$N$ is the total number of data points in the dataset.</li>
</ul>
<h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>Gradient Descent is an optimization algorithm used to minimize the cost function. The idea is to adjust the parameters ($\omega_0$, $\omega_1$, etc.) in the direction of the <strong>negative gradient</strong> of the cost function to reduce the error. The general update rule is:</p>
<p>$$<br>\omega \leftarrow \omega - \lambda \nabla J(\omega)<br>$$</p>
<p>Here:</p>
<ul>
<li>$\omega$ represents the model parameters (e.g., $\omega_0, \omega_1$).</li>
<li>$\lambda$ is the learning rate, controlling the step size in each update.</li>
<li>$\nabla J(\omega)$ is the gradient of the cost function with respect to $\omega$.</li>
</ul>
<hr>
<p>The cost function for linear regression is:</p>
<p>$$<br>J(\omega) &#x3D; \frac{1}{2} \sum_{n&#x3D;1}^N \left( (\omega_0 + \omega_1 x_n) - t_n \right)^2<br>$$</p>
<p>We compute the partial derivatives of $J(\omega)$ with respect to each parameter:</p>
<ol>
<li><p><strong>Gradient with respect to $\omega_0$</strong>:</p>
<p>$$<br>\frac{\partial J(\omega)}{\partial \omega_0} &#x3D; \sum_{n&#x3D;1}^N \left( (\omega_0 + \omega_1 x_n) - t_n \right)<br>$$</p>
</li>
<li><p><strong>Gradient with respect to $\omega_1$</strong>:<br>$$<br>\frac{\partial J(\omega)}{\partial \omega_1} &#x3D; \sum_{n&#x3D;1}^N \left( (\omega_0 + \omega_1 x_n) - t_n \right) x_n<br>$$</p>
</li>
</ol>
<hr>
<p>Using the gradients, the parameters are updated as follows:</p>
<ol>
<li><p><strong>For $\omega_0$</strong>:</p>
<p>$$<br>\omega_0 \leftarrow \omega_0 - \lambda \sum_{n&#x3D;1}^N \left( (\omega_0 + \omega_1 x_n) - t_n \right)<br>$$</p>
</li>
<li><p><strong>For $\omega_1$</strong>:<br>$$<br>\omega_1 \leftarrow \omega_1 - \lambda \sum_{n&#x3D;1}^N \left( (\omega_0 + \omega_1 x_n) - t_n \right) x_n<br>$$</p>
</li>
</ol>
<hr>
<ol>
<li>Initialize $\omega_0$ and $\omega_1$ with some random values (e.g., $0$).</li>
<li>Compute the gradients $\frac{\partial J(\omega)}{\partial \omega_0}$ and $\frac{\partial J(\omega)}{\partial \omega_1}$.</li>
<li>Update $\omega_0$ and $\omega_1$ using the update rules.</li>
<li>Repeat the process until:<ul>
<li>The cost function $J(\omega)$ converges to a minimum, or</li>
<li>The number of iterations reaches a predefined limit.</li>
</ul>
</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://github.com/kongchenglc/Machine-Learning-Examples/blob/master/linear-regression1.py">Demo Code</a></p>
<p><img src="/blog/./img/ML2-1.png"></p>
<h2 id="Linear-Regression-with-Multiple-Features"><a href="#Linear-Regression-with-Multiple-Features" class="headerlink" title="Linear Regression with Multiple Features"></a>Linear Regression with Multiple Features</h2><p>The model can be written as:</p>
<p>$$<br>\hat{y} &#x3D; \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_D x_D<br>$$</p>
<p>Where:</p>
<ul>
<li>$D$ is the number of attributes (features) in the dataset.</li>
<li>$x_1, x_2, \dots, x_D$ are the features of the data.</li>
<li>$\theta_0$ is the intercept (bias term).</li>
<li>$\theta_1, \theta_2, \dots, \theta_D$ are the weights (coefficients) corresponding to each feature.</li>
</ul>
<p>This equation can also be expressed in <strong>matrix form</strong> for computational efficiency.</p>
<hr>
<p>In matrix form, the prediction $\hat{y}$ is expressed as:</p>
<p>$$<br>\hat{y} &#x3D; X \theta<br>$$</p>
<p>Where:</p>
<ul>
<li>$X$ is the design matrix of size $N \times (D+1)$, where:<ul>
<li>$N$ is the number of instances (data points).</li>
<li>The first column of $X$ is all ones, representing the bias term ($\theta_0$).</li>
<li>The remaining columns correspond to the feature values.</li>
</ul>
</li>
</ul>
<p>For example, $X$ can look like this:</p>
<p>$$<br>X &#x3D;<br>\begin{bmatrix}<br>1 &amp; x_1^{(1)} &amp; x_2^{(1)} &amp; \dots &amp; x_D^{(1)} \newline<br>1 &amp; x_1^{(2)} &amp; x_2^{(2)} &amp; \dots &amp; x_D^{(2)} \newline<br>\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \newline<br>1 &amp; x_1^{(N)} &amp; x_2^{(N)} &amp; \dots &amp; x_D^{(N)}<br>\end{bmatrix}<br>$$</p>
<ul>
<li>$\theta$ is the vector of coefficients:<br>$$<br>\theta &#x3D;<br>\begin{bmatrix}<br>\theta_0 \newline<br>\theta_1 \newline<br>\theta_2 \newline<br>\vdots \newline<br>\theta_D<br>\end{bmatrix}<br>$$</li>
</ul>
<hr>
<p>To find the optimal values of $\theta$, we minimize the cost function, which is typically the <strong>Mean Squared Error (MSE)</strong>:</p>
<p>$$<br>J(\theta) &#x3D; \frac{1}{2N} \sum_{i&#x3D;1}^N \left( \hat{y}^{(i)} - t^{(i)} \right)^2<br>$$</p>
<p>In matrix form, this is written as:</p>
<p>$$<br>J(\theta) &#x3D; \frac{1}{2N} | X \theta - t |^2<br>$$</p>
<p>Where $t$ is the vector of true target values:</p>
<p>$$<br>t &#x3D;<br>\begin{bmatrix}<br>t^{(1)} \newline<br>t^{(2)} \newline<br>\vdots \newline<br>t^{(N)}<br>\end{bmatrix}<br>$$</p>
<hr>
<p>By setting the gradient of $J(\theta)$ with respect to $\theta$ to zero, we derive the <strong>closed-form solution</strong>:<br>$$<br>\hat{\theta} &#x3D; (X^T X)^{-1} X^T t<br>$$</p>
<p>Where:</p>
<ul>
<li>$X^T$ is the transpose of the design matrix.</li>
<li>$(X^T X)^{-1}$ is the inverse of the matrix product $X^T X$.</li>
</ul>
<h2 id="Higher-Order-Polynomials"><a href="#Higher-Order-Polynomials" class="headerlink" title="Higher Order Polynomials"></a>Higher Order Polynomials</h2><p>In polynomial regression, we model the relationship between the input variable $x$ and output $f(x, \omega)$ using a higher degree polynomial:</p>
<p>$$<br>f(x, \omega) &#x3D; \omega_0 + \omega_1 x + \omega_2 x^2 + \dots + \omega_M x^M &#x3D; \sum_{j&#x3D;0}^{M} \omega_j x^j<br>$$</p>
<ul>
<li><strong>$x$</strong> is the input feature, and <strong>$\omega_j$</strong> are the coefficients.</li>
<li><strong>$M$</strong> is the degree of the polynomial, which determines the complexity of the model.</li>
</ul>
<p>Choosing $M$ (Degree of Polynomial):</p>
<ul>
<li><strong>Small $M$</strong>: Captures simple relationships; less prone to overfitting.</li>
<li><strong>Large $M$</strong>: Can overfit the data by capturing noise.</li>
<li><strong>Selection</strong>: Cross-validation is used to determine the best $M$ to balance fit and generalization.</li>
</ul>
<p><img src="/blog/./img/ML2-2.png"></p>
<h2 id="Overfitting-Regularization"><a href="#Overfitting-Regularization" class="headerlink" title="Overfitting &amp; Regularization"></a>Overfitting &amp; Regularization</h2><p>In linear regression, <strong>overfitting</strong> occurs when the model becomes too complex and starts to fit noise in the training data. To prevent overfitting, we use <strong>regularization</strong> to penalize large coefficients and simplify the model.</p>
<p>The regularized cost function is:</p>
<p>$$<br>E(\omega) &#x3D; \frac{1}{2} \sum_{n&#x3D;1}^{N} (f(x_n, \omega) - t_n)^2 + \frac{\lambda}{2} |\omega|^2<br>$$</p>
<p>Where:</p>
<ul>
<li><strong>$f(x_n, \omega)$</strong> is the model’s prediction for the $n$-th data point.</li>
<li><strong>$t_n$</strong> is the true target for the $n$-th data point.</li>
<li><strong>$\lambda$</strong> is the regularization parameter that controls the strength of the penalty.</li>
<li><strong>$|\omega|^2$</strong> is the squared <strong>L2 norm</strong> of the weights, i.e., the sum of the squares of the coefficients.</li>
</ul>
<p>The <strong>$\lambda$</strong> parameter allows you to control the trade-off between fitting the data well and keeping the model simple.</p>
<p><img src="/blog/./img/ML2-3.png"></p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/blog/categories/Learning-Notes/page/0/">Previous</a></div><div class="pagination-next"><a href="/blog/categories/Learning-Notes/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/blog/categories/Learning-Notes/">1</a></li><li><a class="pagination-link" href="/blog/categories/Learning-Notes/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/blog/img/profile-pic.jpg" alt="Lich"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Lich</p><p class="is-size-6 is-block">Software Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Canada</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/blog/archives"><p class="title">20</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/blog/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">29</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/kongchenglc" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Defunct Chinese Blog" href="https://kongchenglc.github.io/blog/"><i class="fa-solid fa-link"></i></a></div></div></div><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/">Lich&#039;s Blog</a><p class="is-size-7"><span>&copy; 2025 Lich</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2024</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/blog/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/blog/js/pjax.js"></script><!--!--><script data-pjax src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><!-- hexo injector body_end start --><script src="/blog/js/custom-css.js"></script><!-- hexo injector body_end end --></body></html>