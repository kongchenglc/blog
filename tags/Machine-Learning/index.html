<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Tag: Machine Learning - Cheng&#039;s Blog</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Cheng&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Cheng&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Cheng&#039;s Blog"><meta property="og:url" content="https://kongchenglc.github.io/blog"><meta property="og:site_name" content="Cheng&#039;s Blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://kongchenglc.github.io/blog/img/og_image.png"><meta property="article:author" content="Cheng"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://kongchenglc.github.io/blog/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kongchenglc.github.io/blog"},"headline":"Cheng's Blog","image":["https://kongchenglc.github.io/blog/img/og_image.png"],"author":{"@type":"Person","name":"Cheng"},"publisher":{"@type":"Organization","name":"Cheng's Blog","logo":{"@type":"ImageObject","url":{"text":"Cheng's Blog"}}},"description":""}</script><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/">Cheng&#039;s Blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">Home</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/blog/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">Machine Learning</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-03-11T21:21:59.000Z" title="3/11/2025, 9:21:59 PM">2025-03-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-06-01T01:27:36.336Z" title="6/1/2025, 1:27:36 AM">2025-06-01</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/03/11/Machine-Learning-10/">About Machine Learning ( Part 10: Reinforcement Learning )</a></p><div class="content"><p>Reinforcement Learning (RL) is an exciting field of machine learning where an agent learns how to behave in an environment in order to maximize cumulative rewards. The foundation of RL lies in <strong>Markov Decision Processes (MDPs)</strong>, which provide a mathematical framework for modeling decision-making scenarios. In this blog, we will explore the core concepts of RL and MDPs, and how they interconnect to help an agent learn optimal behavior.</p></div><a class="article-more button is-small is-size-7" href="/blog/2025/03/11/Machine-Learning-10/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-03-11T21:05:34.000Z" title="3/11/2025, 9:05:34 PM">2025-03-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-06-01T01:27:36.338Z" title="6/1/2025, 1:27:36 AM">2025-06-01</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/03/11/Transformer-3/">Transformer ( Part 3: Transformer Architecture )</a></p><div class="content"><h1 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder &amp; Decoder"></a>Encoder &amp; Decoder</h1><p>The Transformer consists of two main parts: an <strong>encoder</strong> and a <strong>decoder</strong>. They are connected by Cross-Attention.</p>
<ul>
<li><strong>Encoder</strong>: Processes the input sequence using multiple layers of self-attention and feed-forward networks.</li>
<li><strong>Decoder</strong>: Takes the encoder’s output and generates the target sequence using self-attention and cross-attention mechanisms.</li>
</ul>
<p>The Transformer Architecture:</p>
<p><img src="/blog/./img/Transformer3-1.png" alt="https://arxiv.org/pdf/1706.03762"></p></div><a class="article-more button is-small is-size-7" href="/blog/2025/03/11/Transformer-3/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-03-01T14:51:44.000Z" title="3/1/2025, 2:51:44 PM">2025-03-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-06-01T01:27:36.337Z" title="6/1/2025, 1:27:36 AM">2025-06-01</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/03/01/Transformer-2/">Transformer ( Part 2: Multi-Head Attention )</a></p><div class="content"><p>Before the Transformer, sequence models like RNNs and LSTMs suffered from <strong>long-term dependency issues</strong> and <strong>low parallelization efficiency</strong>. Self-Attention was introduced as an alternative, allowing for <strong>parallel computation</strong> and capturing <strong>long-range dependencies</strong>.</p>
<p>However, a single-head Self-Attention mechanism has a limitation:<br><strong>It can only focus on one type of relationship or pattern in the data.</strong>  </p>
<p>Multi-Head Attention overcomes this by <strong>using multiple attention heads</strong> that capture different aspects of the input, improving the model’s expressiveness.</p></div><a class="article-more button is-small is-size-7" href="/blog/2025/03/01/Transformer-2/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-12T17:00:56.000Z" title="2/12/2025, 5:00:56 PM">2025-02-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-06-01T01:27:36.337Z" title="6/1/2025, 1:27:36 AM">2025-06-01</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/12/Machine-Learning-9/">About Machine Learning ( Part 9: Recurrent Neural Network )</a></p><div class="content"><p>Recurrent Neural Networks (RNNs) are a class of neural networks designed for <strong>sequential data</strong>, making them highly effective for tasks like <strong>natural language processing (NLP), time series prediction, and speech recognition</strong>. Unlike traditional feedforward networks, RNNs maintain a hidden state that captures <strong>temporal dependencies</strong>.</p>
<h2 id="How-RNNs-Work"><a href="#How-RNNs-Work" class="headerlink" title="How RNNs Work"></a>How RNNs Work</h2><p>A traditional <strong>feedforward neural network</strong> processes inputs independently. However, for sequential tasks, the <strong>order</strong> of the data is crucial. <strong>RNNs address this by maintaining a memory of previous inputs through hidden states.</strong></p></div><a class="article-more button is-small is-size-7" href="/blog/2025/02/12/Machine-Learning-9/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-12T16:33:49.000Z" title="2/12/2025, 4:33:49 PM">2025-02-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-06-01T01:27:36.337Z" title="6/1/2025, 1:27:36 AM">2025-06-01</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/12/Machine-Learning-8/">About Machine Learning ( Part 8: Convolution Neural Networks )</a></p><div class="content"><p>Convolutional Neural Networks (CNNs) have revolutionized the field of computer vision, enabling significant advancements in image recognition, object detection, and segmentation tasks. This blog will explore the key concepts behind CNNs and their working principles.</p>
<h2 id="What-is-a-CNN"><a href="#What-is-a-CNN" class="headerlink" title="What is a CNN?"></a>What is a CNN?</h2><p>A Convolutional Neural Network (CNN) is a type of deep learning model specifically designed for processing structured grid data, such as images. Unlike traditional fully connected neural networks, CNNs leverage <strong>convolutional layers</strong> to capture spatial hierarchies in the data.</p></div><a class="article-more button is-small is-size-7" href="/blog/2025/02/12/Machine-Learning-8/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-09T21:03:49.000Z" title="2/9/2025, 9:03:49 PM">2025-02-09</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-06-01T01:27:36.337Z" title="6/1/2025, 1:27:36 AM">2025-06-01</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/09/Transformer-1/">Transformer ( Part 1: Word Embedding )</a></p><div class="content"><p>Word Embedding is one of the most fundamental techniques in Natural Language Processing (NLP). It represents words as continuous vectors in a high-dimensional space, capturing semantic relationships between them.  </p>
<h2 id="Why-Do-We-Need-Word-Embeddings"><a href="#Why-Do-We-Need-Word-Embeddings" class="headerlink" title="Why Do We Need Word Embeddings?"></a>Why Do We Need Word Embeddings?</h2><p>Before word embeddings, one common method to represent words was <strong>One-Hot Encoding</strong>. In this approach, each word is represented as a high-dimensional sparse vector.  </p>
<p>For example, if our vocabulary has 10,000 words, we encode each word as:<br>$$<br>\text{dog} &#x3D; [0, 1, 0, 0, \dots, 0]<br>$$<br>However, this method has significant drawbacks:  </p>
<ol>
<li><strong>High dimensionality</strong> – A large vocabulary results in enormous vectors.  </li>
<li><strong>No semantic similarity</strong> – “dog” and “cat” are conceptually related, but their one-hot vectors are completely different.</li>
</ol>
<p>Word embeddings solve these issues by <strong>learning low-dimensional, dense representations</strong> that encode semantic relationships between words.  </p>
<p><img src="/blog/./img/Transformer1-1.png" alt="https://corpling.hypotheses.org/495"></p></div><a class="article-more button is-small is-size-7" href="/blog/2025/02/09/Transformer-1/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-06T21:43:39.000Z" title="2/6/2025, 9:43:39 PM">2025-02-06</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-06-01T01:27:36.337Z" title="6/1/2025, 1:27:36 AM">2025-06-01</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/06/Machine-Learning-7/">About Machine Learning ( Part 7: Artificial Neural Network )</a></p><div class="content"><h1 id="Bayes’-theorem"><a href="#Bayes’-theorem" class="headerlink" title="Bayes’ theorem"></a>Bayes’ theorem</h1><p>$$<br>P(y|X) &#x3D; \frac{P(X|y) P(y)}{P(X)}<br>$$</p>
<p>where:</p>
<ul>
<li>$P(y|X)$: Posterior probability of class $y$ given input $X$.</li>
<li>$P(X|y)$: Likelihood of seeing $X$ if the class is $y$.</li>
<li>$P(y)$: Prior probability of class $y$.</li>
<li>$P(X)$: Total probability of $X$ (normalization factor).</li>
</ul>
<h2 id="Bayes-Network-Bayesian-Network-BN"><a href="#Bayes-Network-Bayesian-Network-BN" class="headerlink" title="Bayes Network (Bayesian Network, BN)"></a>Bayes Network (Bayesian Network, BN)</h2><p>A <strong>Bayesian network (BN)</strong> is a <strong>graphical model</strong> representing probabilistic dependencies between variables. It consists of:</p>
<ul>
<li><strong>Nodes:</strong> Represent variables (e.g., symptoms, diseases).</li>
<li><strong>Edges:</strong> Represent conditional dependencies.</li>
</ul>
<p><img src="/blog/./img/ML7-1.png"></p></div><a class="article-more button is-small is-size-7" href="/blog/2025/02/06/Machine-Learning-7/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-02T18:20:57.000Z" title="2/2/2025, 6:20:57 PM">2025-02-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-06-01T01:27:36.337Z" title="6/1/2025, 1:27:36 AM">2025-06-01</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/02/Machine-Learning-6/">About Machine Learning ( Part 6: KNN vs. K-means )</a></p><div class="content"><p>In machine learning, <strong>K-Nearest Neighbors (KNN)</strong> and <strong>K-means Clustering</strong> are two commonly used algorithms. Despite their similar names, they serve <strong>different purposes</strong> and have <strong>distinct working principles</strong>.  </p>
<h1 id="KNN-K-Nearest-Neighbors"><a href="#KNN-K-Nearest-Neighbors" class="headerlink" title="KNN (K-Nearest Neighbors)"></a>KNN (K-Nearest Neighbors)</h1><p>KNN is a <strong>supervised learning</strong> algorithm used for <strong>classification</strong> and <strong>regression</strong> tasks.  </p>
<p>The core idea of KNN is:</p>
<blockquote>
<p>Given a new data point, find the <strong>K</strong> most similar instances in the training dataset (neighbors) and use them to predict the output.</p>
</blockquote>
<p>KNN is a <strong>lazy learning</strong> algorithm, meaning it does not require a training phase. Instead, it directly classifies or predicts based on stored data.</p></div><a class="article-more button is-small is-size-7" href="/blog/2025/02/02/Machine-Learning-6/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-02T15:50:57.000Z" title="2/2/2025, 3:50:57 PM">2025-02-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-06-01T01:27:36.337Z" title="6/1/2025, 1:27:36 AM">2025-06-01</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/02/02/Machine-Learning-5/">About Machine Learning ( Part 5: Support Vector Machine )</a></p><div class="content"><h1 id="Support-Vector-Machine-SVM"><a href="#Support-Vector-Machine-SVM" class="headerlink" title="Support Vector Machine (SVM)"></a>Support Vector Machine (SVM)</h1><p>Support Vector Machines (SVM) are one of the most powerful supervised learning algorithms used for classification and regression tasks.</p>
<h2 id="The-Hyperplane"><a href="#The-Hyperplane" class="headerlink" title="The Hyperplane"></a>The Hyperplane</h2><p>In a binary classification problem, the goal of SVM is to find a <strong>hyperplane</strong> that best separates two classes. Given a training dataset:</p>
<p>$$<br>D &#x3D; { (\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \dots, (\mathbf{x}_n, y_n) }, \quad \mathbf{x}_i \in \mathbb{R}^d, \quad y_i \in {-1, +1}<br>$$</p>
<ul>
<li>$\mathbf{x}_i$: $d$-dimensional feature vector (e.g., pixel values in an image).</li>
<li>$y_i$: Class label ($+1$ for “cat”, $-1$ for “dog”).</li>
</ul></div><a class="article-more button is-small is-size-7" href="/blog/2025/02/02/Machine-Learning-5/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-01-21T15:50:57.000Z" title="1/21/2025, 3:50:57 PM">2025-01-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-06-01T01:27:36.336Z" title="6/1/2025, 1:27:36 AM">2025-06-01</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/2025/01/21/Machine-Learning-4/">About Machine Learning ( Part 4: Decision Tree )</a></p><div class="content"><p>A Decision Tree is a supervised learning algorithm used for both classification and regression tasks. It organizes data into a tree-like structure, where each internal node represents a decision based on a feature, and each leaf node provides a prediction. Decision trees are simple, interpretable, and capable of handling both categorical and numerical data.</p>
<h1 id="Classification-Tree"><a href="#Classification-Tree" class="headerlink" title="Classification Tree"></a>Classification Tree</h1><p>A <strong>Classification Tree</strong> is a decision tree used for classifying data into distinct categories or classes. The main objective of a classification tree is to predict the category or class to which a given input belongs based on various features.</p>
<p><img src="/blog/./img/ML4-1.png"></p></div><a class="article-more button is-small is-size-7" href="/blog/2025/01/21/Machine-Learning-4/#more">Read more</a></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/blog/tags/Machine-Learning/page/0/">Previous</a></div><div class="pagination-next"><a href="/blog/tags/Machine-Learning/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/blog/tags/Machine-Learning/">1</a></li><li><a class="pagination-link" href="/blog/tags/Machine-Learning/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/blog/img/profile-pic.jpg" alt="Cheng"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Cheng</p><p class="is-size-6 is-block">Software Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Canada</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/blog/archives"><p class="title">22</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/blog/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">32</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/kongchenglc" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Linkedin" href="https://www.linkedin.com/in/kongchenglc/"><i class="fab fa-linkedin"></i></a></div></div></div><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/">Cheng&#039;s Blog</a><p class="is-size-7"><span>&copy; 2025 Cheng</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2024</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/blog/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/blog/js/pjax.js"></script><!--!--><script data-pjax src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><!-- hexo injector body_end start --><script src="/blog/js/custom-css.js"></script><!-- hexo injector body_end end --></body></html>