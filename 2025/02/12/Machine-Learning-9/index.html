<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>About Machine Learning ( Part 9: Recurrent Neural Network ) - Cheng&#039;s Blog</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Cheng&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Cheng&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Recurrent Neural Networks (RNNs) are a class of neural networks designed for sequential data, making them highly effective for tasks like natural language processing (NLP), time series prediction, and"><meta property="og:type" content="blog"><meta property="og:title" content="About Machine Learning ( Part 9: Recurrent Neural Network )"><meta property="og:url" content="https://kongchenglc.github.io/blog/2025/02/12/Machine-Learning-9/"><meta property="og:site_name" content="Cheng&#039;s Blog"><meta property="og:description" content="Recurrent Neural Networks (RNNs) are a class of neural networks designed for sequential data, making them highly effective for tasks like natural language processing (NLP), time series prediction, and"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://kongchenglc.github.io/blog/img/ML9-1.png"><meta property="og:image" content="https://kongchenglc.github.io/blog/img/ML9-2.png"><meta property="og:image" content="https://kongchenglc.github.io/blog/img/ML9-3.png"><meta property="article:published_time" content="2025-02-12T17:00:56.000Z"><meta property="article:modified_time" content="2025-07-07T19:30:55.323Z"><meta property="article:author" content="Cheng"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Artificial Neural Network (ANN)"><meta property="article:tag" content="Recurrent Neural Network"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://kongchenglc.github.io/blog/img/ML9-1.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kongchenglc.github.io/blog/2025/02/12/Machine-Learning-9/"},"headline":"About Machine Learning ( Part 9: Recurrent Neural Network )","image":["https://kongchenglc.github.io/blog/img/ML9-1.png","https://kongchenglc.github.io/blog/img/ML9-2.png","https://kongchenglc.github.io/blog/img/ML9-3.png"],"datePublished":"2025-02-12T17:00:56.000Z","dateModified":"2025-07-07T19:30:55.323Z","author":{"@type":"Person","name":"Cheng"},"publisher":{"@type":"Organization","name":"Cheng's Blog","logo":{"@type":"ImageObject","url":{"text":"Cheng's Blog"}}},"description":"Recurrent Neural Networks (RNNs) are a class of neural networks designed for sequential data, making them highly effective for tasks like natural language processing (NLP), time series prediction, and"}</script><link rel="canonical" href="https://kongchenglc.github.io/blog/2025/02/12/Machine-Learning-9/"><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/">Cheng&#039;s Blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">Home</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-02-12T17:00:56.000Z" title="2/12/2025, 5:00:56 PM">2025-02-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-07-07T19:30:55.323Z" title="7/7/2025, 7:30:55 PM">2025-07-07</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><h1 class="title is-3 is-size-4-mobile">About Machine Learning ( Part 9: Recurrent Neural Network )</h1><div class="content"><p>Recurrent Neural Networks (RNNs) are a class of neural networks designed for <strong>sequential data</strong>, making them highly effective for tasks like <strong>natural language processing (NLP), time series prediction, and speech recognition</strong>. Unlike traditional feedforward networks, RNNs maintain a hidden state that captures <strong>temporal dependencies</strong>.</p>
<h2 id="How-RNNs-Work"><a href="#How-RNNs-Work" class="headerlink" title="How RNNs Work"></a>How RNNs Work</h2><p>A traditional <strong>feedforward neural network</strong> processes inputs independently. However, for sequential tasks, the <strong>order</strong> of the data is crucial. <strong>RNNs address this by maintaining a memory of previous inputs through hidden states.</strong></p>
<span id="more"></span>

<h3 id="Mathematical-Representation"><a href="#Mathematical-Representation" class="headerlink" title="Mathematical Representation"></a>Mathematical Representation</h3><p>At each time step $t$, an RNN takes the <strong>current input</strong> $x_t$ and the <strong>previous hidden state</strong> $h_{t-1}$ to compute the <strong>new hidden state</strong> $h_t$:</p>
<p>$$<br>h_t &#x3D; f(W_h h_{t-1} + W_x x_t + b_h)<br>$$</p>
<p>where:</p>
<ul>
<li>$W_h$ and $W_x$ are weight matrices,</li>
<li>$b_h$ is the bias term,</li>
<li>$f$ is usually a non-linear activation function like $ \tanh $,</li>
<li>$h_t$ represents the <strong>memory</strong> of past computations.</li>
</ul>
<p>Finally, the <strong>output</strong> $o_t$ is computed as:</p>
<p>$$<br>o_t &#x3D; g(W_y h_t + b_y)<br>$$</p>
<p>where $g$ is often a softmax function for classification tasks.</p>
<p><img src="/blog/./img/ML9-1.png" alt="http://dprogrammer.org/rnn-lstm-gru"></p>
<h2 id="The-Vanishing-Gradient-Problem"><a href="#The-Vanishing-Gradient-Problem" class="headerlink" title="The Vanishing Gradient Problem"></a>The Vanishing Gradient Problem</h2><p>A major challenge in training RNNs is the <strong>vanishing gradient problem</strong>.<br>During backpropagation, gradients can shrink exponentially when passing through many time steps, making it difficult to <strong>learn long-range dependencies</strong>.</p>
<p>To address this issue, <strong>LSTMs (Long Short-Term Memory)</strong> and <strong>GRUs (Gated Recurrent Units)</strong> were introduced.</p>
<h2 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long Short-Term Memory (LSTM)"></a>Long Short-Term Memory (LSTM)</h2><p>LSTMs introduce <strong>gates</strong> to control how much past information should be retained or discarded. This helps <strong>preserve long-term dependencies</strong>.</p>
<p>Each LSTM unit has:</p>
<ol>
<li><strong>Forget Gate</strong>: Decides what information to discard<br>$$<br>f_t &#x3D; \sigma(W_f h_{t-1} + U_f x_t + b_f)<br>$$</li>
<li><strong>Input Gate</strong>: Determines new information to store<br>$$<br>i_t &#x3D; \sigma(W_i h_{t-1} + U_i x_t + b_i)<br>$$</li>
<li><strong>Cell State Update</strong>: Computes the candidate memory</li>
</ol>
<p>$$<br>\tilde{C_t} &#x3D; \tanh(W_c h_{t-1} + U_c x_t + b_c)<br>$$</p>
<p>Then updates the cell state:</p>
<p>$$<br>C_t &#x3D; f_t C_{t-1} + i_t \tilde{C}_t<br>$$</p>
<ol start="4">
<li><strong>Output Gate</strong>: Determines the final hidden state<br>$$<br>o_t &#x3D; \sigma(W_o h_{t-1} + U_o x_t + b_o)<br>$$<br>$$<br>h_t &#x3D; o_t \tanh(C_t)<br>$$</li>
</ol>
<p>LSTMs ensure that important past information <strong>remains accessible over long sequences</strong>.</p>
<p><img src="/blog/./img/ML9-2.png" alt="http://dprogrammer.org/rnn-lstm-gru"></p>
<h3 id="Cell-State-C-t"><a href="#Cell-State-C-t" class="headerlink" title="Cell State $C_t$"></a>Cell State $C_t$</h3><p>The <strong>cell state</strong> can be considered as the “memory” of the LSTM network. It carries information across time steps, and it’s responsible for helping the network preserve <strong>long-term dependencies</strong>. The cell state is essentially the backbone of an LSTM that allows it to remember information over long sequences.</p>
<h4 id="Update-Mechanism-of-Cell-State"><a href="#Update-Mechanism-of-Cell-State" class="headerlink" title="Update Mechanism of Cell State"></a>Update Mechanism of Cell State</h4><p>The cell state is updated through two important gates in the LSTM:</p>
<ol>
<li><strong>Forget Gate</strong> ($f_t$) - Decides what information should be discarded.</li>
<li><strong>Input Gate</strong> ($i_t$) - Determines what new information should be added to the memory.</li>
</ol>
<p>The cell state $C_t$ is updated as follows:</p>
<p>$$<br>C_t &#x3D; f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t<br>$$</p>
<p>Where:</p>
<ul>
<li>$C_{t-1}$ is the previous cell state.</li>
<li>$f_t$ is the forget gate’s output, which decides how much of the previous cell state should be remembered.</li>
<li>$i_t$ is the input gate’s output, which decides how much new information should be stored in the cell state.</li>
<li>$\tilde{C}_t$ is the candidate cell state, which represents the new potential memory.</li>
</ul>
<h3 id="Hidden-State-h-t"><a href="#Hidden-State-h-t" class="headerlink" title="Hidden State $h_t$"></a>Hidden State $h_t$</h3><p>The <strong>hidden state</strong> is the network’s <strong>short-term memory</strong>. It represents the <strong>output</strong> of the LSTM at each time step and carries information relevant to the current time step’s computation. The hidden state is the value that is passed to the next time step and is typically used to generate predictions or outputs.</p>
<h4 id="Update-Mechanism-of-Hidden-State"><a href="#Update-Mechanism-of-Hidden-State" class="headerlink" title="Update Mechanism of Hidden State"></a>Update Mechanism of Hidden State</h4><p>The hidden state $h_t$ is derived from the current cell state $C_t$ using the <strong>output gate</strong> ($o_t$). The output gate decides how much of the current cell state should be exposed as the hidden state:</p>
<p>$$<br>h_t &#x3D; o_t \cdot \tanh(C_t)<br>$$</p>
<p>Where:</p>
<ul>
<li>$o_t$ is the output gate, which determines how much of the cell state should be visible in the hidden state.</li>
<li>$\tanh(C_t)$ is the cell state passed through the $\tanh$ activation function.</li>
</ul>
<h3 id="Differences-Between-C-t-and-h-t"><a href="#Differences-Between-C-t-and-h-t" class="headerlink" title="Differences Between $C_t$ and $h_t$"></a>Differences Between $C_t$ and $h_t$</h3><p>While both <strong>cell state</strong> ($C_t$) and <strong>hidden state</strong> ($h_t$) are crucial in LSTM networks, they serve distinct roles:</p>
<ul>
<li><p><strong>$C_t$ (Cell State)</strong>: </p>
<ul>
<li>Acts as the <strong>long-term memory</strong> of the network.</li>
<li>It is designed to carry information over long time periods, ensuring the network remembers relevant data from earlier time steps.</li>
<li>The cell state is passed through the time steps with minimal changes unless explicitly modified by the forget and input gates.</li>
</ul>
</li>
<li><p><strong>$h_t$ (Hidden State)</strong>:</p>
<ul>
<li>Acts as the <strong>short-term memory</strong>.</li>
<li>It contains information that is relevant to the current time step and is used to generate the output of the LSTM at each time step.</li>
<li>The hidden state is passed to the next time step as the updated memory, which is used for prediction or classification.</li>
</ul>
</li>
</ul>
<h2 id="Gated-Recurrent-Unit-GRU"><a href="#Gated-Recurrent-Unit-GRU" class="headerlink" title="Gated Recurrent Unit (GRU)"></a>Gated Recurrent Unit (GRU)</h2><p>GRUs are another variant of RNNs introduced to improve the learning of long-range dependencies. GRUs are <strong>simpler</strong> than LSTMs, as they use fewer gates, which can lead to faster training times while still addressing the vanishing gradient problem.</p>
<p><img src="/blog/./img/ML9-3.png" alt="http://dprogrammer.org/rnn-lstm-gru"></p>
<p>A GRU unit consists of two main gates:</p>
<ol>
<li><p><strong>Update Gate</strong>: This gate decides how much of the past information should be carried forward.<br>$$<br>z_t &#x3D; \sigma(W_z h_{t-1} + U_z x_t + b_z)<br>$$</p>
<p> $z_t &#x3D; 0$: The model keeps the previous hidden state $h_{t-1}$ and does not update with new information.<br> $z_t &#x3D; 1$: The model fully updates the hidden state with the new candidate hidden state $\tilde{h}_t$.</p>
</li>
<li><p><strong>Reset Gate</strong>: This gate determines how much of the past hidden state should be forgotten.<br>$$<br>r_t &#x3D; \sigma(W_r h_{t-1} + U_r x_t + b_r)<br>$$</p>
<p> $r_t &#x3D; 0$: Forget the previous hidden state.<br> $r_t &#x3D; 1$: Use the previous hidden state fully.</p>
</li>
</ol>
<p>The candidate hidden state $\tilde{h}_t$ is computed as:</p>
<p>$$<br>\tilde{h_t} &#x3D; \tanh(W_h (r_t \cdot h_{t-1}) + U_h x_t + b_h)<br>$$</p>
<p>Finally, the hidden state $h_t$ is updated as a combination of the previous hidden state and the candidate hidden state:</p>
<p>$$<br>h_t &#x3D; (1 - z_t) \cdot h_{t-1} + z_t \cdot \tilde{h}_t<br>$$</p>
<p>The <strong>update gate</strong> controls how much of the previous hidden state is kept, while the <strong>reset gate</strong> determines how much of the past information is discarded.</p>
<p>GRUs are computationally more efficient than LSTMs because they have fewer parameters to train, yet often perform comparably in many tasks.</p>
<h2 id="LSTM-vs-GRU-Key-Differences"><a href="#LSTM-vs-GRU-Key-Differences" class="headerlink" title="LSTM vs. GRU: Key Differences"></a>LSTM vs. GRU: Key Differences</h2><p>While both LSTMs and GRUs address the vanishing gradient problem, their architecture and gate structure differ:</p>
<ol>
<li><p><strong>Number of Gates</strong>:</p>
<ul>
<li><strong>LSTM</strong> has <strong>three gates</strong>: Forget gate, Input gate, and Output gate.</li>
<li><strong>GRU</strong> has only <strong>two gates</strong>: Update gate and Reset gate.</li>
</ul>
</li>
<li><p><strong>Complexity</strong>:</p>
<ul>
<li><strong>LSTM</strong> is generally <strong>more complex</strong> and has more parameters due to the extra gates and the cell state.</li>
<li><strong>GRU</strong> is <strong>simpler</strong> with fewer parameters, making it computationally more efficient.</li>
</ul>
</li>
<li><p><strong>Performance</strong>:</p>
<ul>
<li><strong>LSTM</strong> might perform better on some tasks due to its ability to separately maintain the cell state and hidden state, but it may require more data and computational resources.</li>
<li><strong>GRU</strong>, being simpler, can often match or even outperform LSTM on smaller datasets or simpler tasks.</li>
</ul>
</li>
<li><p><strong>Use Cases</strong>:</p>
<ul>
<li><strong>LSTM</strong> is typically used when the task requires more complex learning of long-term dependencies.</li>
<li><strong>GRU</strong> is useful for faster training and can be an effective choice for tasks where slightly fewer gates might still work well.</li>
</ul>
</li>
</ol>
<p>In many practical scenarios, both LSTM and GRU can give similar results. The choice between the two often comes down to the specific task, available computational resources, and performance requirements.</p>
<h2 id="Applications-of-RNNs"><a href="#Applications-of-RNNs" class="headerlink" title="Applications of RNNs"></a>Applications of RNNs</h2><p>RNNs and their variants (LSTM, GRU) are widely used in:</p>
<p><strong>Natural Language Processing (NLP)</strong></p>
<ul>
<li>Sentiment analysis</li>
<li>Machine translation (Google Translate)</li>
<li>Chatbots &amp; conversational AI</li>
</ul>
<p><strong>Time Series Forecasting</strong></p>
<ul>
<li>Stock price prediction</li>
<li>Weather forecasting</li>
</ul>
<p><strong>Speech Recognition</strong></p>
<ul>
<li>Voice assistants like Siri &amp; Google Assistant</li>
</ul>
<p><strong>Music Generation</strong></p>
<ul>
<li>AI-generated compositions</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>About Machine Learning ( Part 9: Recurrent Neural Network )</p><p><a href="https://kongchenglc.github.io/blog/2025/02/12/Machine-Learning-9/">https://kongchenglc.github.io/blog/2025/02/12/Machine-Learning-9/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Cheng</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2025-02-12</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2025-07-07</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/Machine-Learning/">Machine Learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/Artificial-Neural-Network-ANN/">Artificial Neural Network (ANN)</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/Recurrent-Neural-Network/">Recurrent Neural Network</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/2025/03/01/Transformer-2/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Transformer ( Part 2: Multi-Head Attention )</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2025/02/12/Machine-Learning-8/"><span class="level-item">About Machine Learning ( Part 8: Convolution Neural Networks )</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><script src="https://utteranc.es/client.js" repo="kongchenglc/blog" issue-term="pathname" label="comment" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/blog/img/profile-pic.jpg" alt="Cheng"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Cheng</p><p class="is-size-6 is-block">Software Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Canada</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/blog/archives"><p class="title">23</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/blog/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">38</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/kongchenglc" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Linkedin" href="https://www.linkedin.com/in/kongchenglc/"><i class="fab fa-linkedin"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#How-RNNs-Work"><span class="level-left"><span class="level-item">1</span><span class="level-item">How RNNs Work</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Mathematical-Representation"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">Mathematical Representation</span></span></a></li></ul></li><li><a class="level is-mobile" href="#The-Vanishing-Gradient-Problem"><span class="level-left"><span class="level-item">2</span><span class="level-item">The Vanishing Gradient Problem</span></span></a></li><li><a class="level is-mobile" href="#Long-Short-Term-Memory-LSTM"><span class="level-left"><span class="level-item">3</span><span class="level-item">Long Short-Term Memory (LSTM)</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Cell-State-C-t"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">Cell State $C_t$</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Update-Mechanism-of-Cell-State"><span class="level-left"><span class="level-item">3.1.1</span><span class="level-item">Update Mechanism of Cell State</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Hidden-State-h-t"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">Hidden State $h_t$</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Update-Mechanism-of-Hidden-State"><span class="level-left"><span class="level-item">3.2.1</span><span class="level-item">Update Mechanism of Hidden State</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Differences-Between-C-t-and-h-t"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">Differences Between $C_t$ and $h_t$</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Gated-Recurrent-Unit-GRU"><span class="level-left"><span class="level-item">4</span><span class="level-item">Gated Recurrent Unit (GRU)</span></span></a></li><li><a class="level is-mobile" href="#LSTM-vs-GRU-Key-Differences"><span class="level-left"><span class="level-item">5</span><span class="level-item">LSTM vs. GRU: Key Differences</span></span></a></li><li><a class="level is-mobile" href="#Applications-of-RNNs"><span class="level-left"><span class="level-item">6</span><span class="level-item">Applications of RNNs</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/blog/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/">Cheng&#039;s Blog</a><p class="is-size-7"><span>&copy; 2025 Cheng</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2024</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/blog/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><!--!--><script data-pjax src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><!-- hexo injector body_end start --><script src="/blog/js/custom-css.js"></script><!-- hexo injector body_end end --></body></html>