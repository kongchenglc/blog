<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>About Machine Learning ( Part 10: Reinforcement Learning ) - Mike&#039;s Blog</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Mike&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Mike&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="IntroductionReinforcement Learning (RL) is a fascinating branch of machine learning where an agent learns to interact with an environment to maximize long-term cumulative rewards. Unlike supervised le"><meta property="og:type" content="blog"><meta property="og:title" content="About Machine Learning ( Part 10: Reinforcement Learning )"><meta property="og:url" content="https://kongchenglc.github.io/blog/2025/03/11/Machine-Learning-10/"><meta property="og:site_name" content="Mike&#039;s Blog"><meta property="og:description" content="IntroductionReinforcement Learning (RL) is a fascinating branch of machine learning where an agent learns to interact with an environment to maximize long-term cumulative rewards. Unlike supervised le"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://kongchenglc.github.io/blog/img/ML10-1.png"><meta property="article:published_time" content="2025-03-11T21:21:59.000Z"><meta property="article:modified_time" content="2025-07-25T22:08:39.225Z"><meta property="article:author" content="Cheng (Mike)"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="GYM"><meta property="article:tag" content="Actor-Critic"><meta property="article:tag" content="PPO"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://kongchenglc.github.io/blog/img/ML10-1.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kongchenglc.github.io/blog/2025/03/11/Machine-Learning-10/"},"headline":"About Machine Learning ( Part 10: Reinforcement Learning )","image":["https://kongchenglc.github.io/blog/img/ML10-1.png"],"datePublished":"2025-03-11T21:21:59.000Z","dateModified":"2025-07-25T22:08:39.225Z","author":{"@type":"Person","name":"Cheng (Mike)"},"publisher":{"@type":"Organization","name":"Mike's Blog","logo":{"@type":"ImageObject","url":{"text":"Mike's Blog"}}},"description":"IntroductionReinforcement Learning (RL) is a fascinating branch of machine learning where an agent learns to interact with an environment to maximize long-term cumulative rewards. Unlike supervised le"}</script><link rel="canonical" href="https://kongchenglc.github.io/blog/2025/03/11/Machine-Learning-10/"><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/">Mike&#039;s Blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">Home</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-03-11T21:21:59.000Z" title="3/11/2025, 9:21:59 PM">2025-03-11</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><h1 class="title is-3 is-size-4-mobile">About Machine Learning ( Part 10: Reinforcement Learning )</h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><strong>Reinforcement Learning (RL)</strong> is a fascinating branch of machine learning where an agent learns to interact with an environment to maximize long-term cumulative rewards. Unlike supervised learning, RL relies on feedback through interaction instead of labeled data.</p>
<p>The core of RL is built upon <strong>Markov Decision Processes (MDPs)</strong>, which provide a mathematical framework for modeling decision-making under uncertainty.</p>
<p>This blog post explores the key components of RL, including value functions, Q-functions, the Bellman equation, Actor-Critic architectures, PPO, and commonly used tools in real-world RL implementations.</p>
<span id="more"></span>

<h1 id="What-is-Reinforcement-Learning"><a href="#What-is-Reinforcement-Learning" class="headerlink" title="What is Reinforcement Learning?"></a>What is Reinforcement Learning?</h1><p>Reinforcement Learning involves an agent that:</p>
<ul>
<li>Observes a <strong>state</strong> $s$</li>
<li>Takes an <strong>action</strong> $a$</li>
<li>Receives a <strong>reward</strong> $r$</li>
<li>Transitions to a new state $sâ€™$</li>
<li>Updates its <strong>policy</strong> $\pi$ to maximize long-term rewards</li>
</ul>
<h2 id="Key-Concepts"><a href="#Key-Concepts" class="headerlink" title="Key Concepts"></a>Key Concepts</h2><table>
<thead>
<tr>
<th>Term</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Agent</strong></td>
<td>The learner or decision-maker</td>
</tr>
<tr>
<td><strong>Environment</strong></td>
<td>The system the agent interacts with</td>
</tr>
<tr>
<td><strong>State ($s$)</strong></td>
<td>The current situation</td>
</tr>
<tr>
<td><strong>Action ($a$)</strong></td>
<td>A decision taken by the agent</td>
</tr>
<tr>
<td><strong>Reward ($r$)</strong></td>
<td>A scalar feedback signal from the environment</td>
</tr>
<tr>
<td><strong>Policy ($\pi$)</strong></td>
<td>A strategy mapping states to actions</td>
</tr>
<tr>
<td><strong>Value Function ($V(s)$)</strong></td>
<td>Expected return from state $s$</td>
</tr>
<tr>
<td><strong>Q-Function ($Q(s,a)$)</strong></td>
<td>Expected return from taking action $a$ in state $s$</td>
</tr>
</tbody></table>
<p><img src="/blog/./img/ML10-1.png" alt="https://research.google/blog/off-policy-estimation-for-infinite-horizon-reinforcement-learning/"></p>
<h1 id="Exploration-vs-Exploitation"><a href="#Exploration-vs-Exploitation" class="headerlink" title="Exploration vs. Exploitation"></a>Exploration vs. Exploitation</h1><p>In RL, the agent must balance:</p>
<ul>
<li><strong>Exploration</strong>: Trying unknown actions to learn about the environment</li>
<li><strong>Exploitation</strong>: Choosing the best-known action to maximize reward</li>
</ul>
<p>This balance is crucial for learning an optimal policy without getting stuck in local optima.</p>
<h1 id="Markov-Decision-Process-MDP"><a href="#Markov-Decision-Process-MDP" class="headerlink" title="Markov Decision Process (MDP)"></a>Markov Decision Process (MDP)</h1><p>An <strong>MDP</strong> formally defines an RL problem using:</p>
<ol>
<li><strong>States ($S$)</strong>: Set of all possible states</li>
<li><strong>Actions ($A$)</strong>: Set of all possible actions</li>
<li><strong>Transition Probability ($P(sâ€™|s,a)$)</strong>: Probability of transitioning to state $sâ€™$ from state $s$ after action $a$</li>
<li><strong>Reward Function ($R(s,a)$)</strong>: Reward received when taking action $a$ in state $s$</li>
<li><strong>Discount Factor ($\gamma$)</strong>: A number in $[0, 1]$ that determines how much future rewards are worth</li>
</ol>
<h2 id="Markov-Property"><a href="#Markov-Property" class="headerlink" title="Markov Property"></a>Markov Property</h2><p>An MDP assumes the <strong>Markov property</strong>:</p>
<blockquote>
<p>The future is independent of the past given the present.</p>
</blockquote>
<p>This means the next state depends only on the current state and action, not on the sequence of events that preceded it.</p>
<h1 id="Value-Function-and-Q-Function"><a href="#Value-Function-and-Q-Function" class="headerlink" title="Value Function and Q-Function"></a>Value Function and Q-Function</h1><h2 id="State-Value-Function-V-s"><a href="#State-Value-Function-V-s" class="headerlink" title="State Value Function $V(s)$"></a>State Value Function $V(s)$</h2><p>The expected return starting from state $s$ and following policy $\pi$ is:</p>
<p>$$<br>V(s) &#x3D; \mathbb{E} \left[ \sum_{t&#x3D;0}^{\infty} \gamma^t r_t \Big| s_0 &#x3D; s \right]<br>$$</p>
<ul>
<li>$r_t$: reward at time step $t$</li>
<li>$\gamma$: discount factor</li>
<li>$\mathbb{E}_\pi$: expectation under policy $\pi$</li>
</ul>
<h2 id="Action-Value-Function-Q-s-a"><a href="#Action-Value-Function-Q-s-a" class="headerlink" title="Action-Value Function $Q(s, a)$"></a>Action-Value Function $Q(s, a)$</h2><p>The expected return from state $s$, taking action $a$, and following policy $\pi$ is:</p>
<p>$$<br>Q(s, a) &#x3D; \mathbb{E} \left[ \sum_{t&#x3D;0}^{\infty} \gamma^t r_t \Big| s_0 &#x3D; s, a_0 &#x3D; a \right]<br>$$</p>
<ul>
<li>$a_0$: action taken at $t&#x3D;0$</li>
</ul>
<h2 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h2><p>The <strong>Bellman Equation</strong> defines a recursive relationship for value functions.</p>
<h3 id="For-the-value-function"><a href="#For-the-value-function" class="headerlink" title="For the value function:"></a>For the value function:</h3><p>$$<br>V(s) &#x3D; \max_a \left( R(s, a) + \gamma \sum_{sâ€™} P(sâ€™|s, a) V(sâ€™) \right)<br>$$</p>
<ul>
<li>$R(s,a)$: immediate reward</li>
<li>$P(sâ€™|s,a)$: transition probability</li>
<li>$V(sâ€™)$: value of the next state</li>
</ul>
<h3 id="For-the-Q-function"><a href="#For-the-Q-function" class="headerlink" title="For the Q-function:"></a>For the Q-function:</h3><p>$$<br>Q(s, a) &#x3D; R(s, a) + \gamma \sum_{sâ€™} P(sâ€™|s, a) \max_{aâ€™} Q(sâ€™, aâ€™)<br>$$</p>
<ul>
<li>$Q(sâ€™, aâ€™)$: estimated future return of next state and action</li>
</ul>
<h1 id="Actor-Critic-Architecture"><a href="#Actor-Critic-Architecture" class="headerlink" title="Actor-Critic Architecture"></a>Actor-Critic Architecture</h1><p><strong>Actor-Critic</strong> methods use:</p>
<ul>
<li><strong>Actor</strong>: Chooses actions based on policy $\pi$</li>
<li><strong>Critic</strong>: Evaluates the chosen action using $V(s)$ or $Q(s,a)$</li>
</ul>
<p>This separation helps when:</p>
<ul>
<li>Rewards are sparse or delayed</li>
<li>The environment is complex</li>
</ul>
<p>The Critic provides guidance, helping the Actor improve its policy more efficiently.</p>
<h1 id="PPO-Proximal-Policy-Optimization"><a href="#PPO-Proximal-Policy-Optimization" class="headerlink" title="PPO (Proximal Policy Optimization)"></a>PPO (Proximal Policy Optimization)</h1><p><strong>PPO</strong> is a modern Actor-Critic-based policy optimization algorithm that stabilizes learning through <strong>clipped surrogate objectives</strong>.</p>
<h2 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow:"></a>Workflow:</h2><ol>
<li>Interact with environment to collect experiences</li>
<li>Compute advantage estimates</li>
<li>Update Actor using a clipped objective</li>
<li>Update Critic by minimizing value loss</li>
<li>Repeat</li>
</ol>
<h2 id="PPO-Objectives"><a href="#PPO-Objectives" class="headerlink" title="PPO Objectives"></a>PPO Objectives</h2><h3 id="1-Clipped-Policy-Objective"><a href="#1-Clipped-Policy-Objective" class="headerlink" title="1. Clipped Policy Objective:"></a>1. Clipped Policy Objective:</h3><p>$$<br>L^{CLIP}(\theta) &#x3D; \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]<br>$$</p>
<ul>
<li>$r_t(\theta) &#x3D; \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$: policy ratio</li>
<li>$\epsilon$: small hyperparameter controlling clip range</li>
<li>$\hat{A}_t$: advantage estimate</li>
</ul>
<h3 id="2-Value-Function-Loss"><a href="#2-Value-Function-Loss" class="headerlink" title="2. Value Function Loss:"></a>2. Value Function Loss:</h3><p>$$<br>L^{VF}(\theta) &#x3D; \mathbb{E}_t \left[ \left( V_\theta(s_t) - \hat{R}_t \right)^2 \right]<br>$$</p>
<ul>
<li>$V_\theta(s_t)$: predicted value</li>
<li>$\hat{R}_t$: actual return</li>
</ul>
<h3 id="3-Total-Objective-with-Entropy-Regularization"><a href="#3-Total-Objective-with-Entropy-Regularization" class="headerlink" title="3. Total Objective with Entropy Regularization:"></a>3. Total Objective with Entropy Regularization:</h3><p>$$<br>L(\theta) &#x3D; L^{CLIP}(\theta) - c_1 L^{VF}(\theta) + c_2 S[\pi_\theta] (s_t)<br>$$</p>
<ul>
<li>$S\left[\pi_\theta\right] (s_t)$: policy entropy for encouraging exploration</li>
<li>$c_1$, $c_2$: tuning coefficients</li>
</ul>
<h2 id="Why-Use-Advantage-Instead-of-Raw-Reward"><a href="#Why-Use-Advantage-Instead-of-Raw-Reward" class="headerlink" title="Why Use Advantage Instead of Raw Reward?"></a>Why Use Advantage Instead of Raw Reward?</h2><p>Using total return $\hat{R}_t$ directly to update the policy may be misleading due to:</p>
<ol>
<li><strong>State quality bias</strong>: Some states are inherently good&#x2F;bad regardless of the action</li>
<li><strong>High variance</strong>: Makes learning unstable</li>
</ol>
<p>Instead, we compute the <strong>Advantage Function</strong>:</p>
<p>$$<br>\hat{A}_t &#x3D; \hat{R}_t - V(s_t)<br>$$</p>
<ul>
<li>$\hat{R}_t$: estimated return from time $t$</li>
<li>$V(s_t)$: criticâ€™s estimate of state value</li>
</ul>
<p>This tells us whether the action was <strong>better or worse than expected</strong> in a given state.</p>
<h2 id="What-is-KL-Divergence"><a href="#What-is-KL-Divergence" class="headerlink" title="What is KL Divergence?"></a>What is KL Divergence?</h2><p><strong>Kullback-Leibler (KL) Divergence</strong> measures how much one probability distribution diverges from another.</p>
<p>In PPO, it is used to:</p>
<ul>
<li>Monitor how much the new policy has changed from the old one</li>
<li>Trigger early stopping or adjust learning rate if divergence is too high</li>
</ul>
<h1 id="GYM-Gymnasium"><a href="#GYM-Gymnasium" class="headerlink" title="GYM &#x2F; Gymnasium"></a>GYM &#x2F; Gymnasium</h1><p><strong>Gymnasium</strong> is a toolkit for building and testing RL environments.</p>
<h2 id="Key-Features"><a href="#Key-Features" class="headerlink" title="Key Features:"></a>Key Features:</h2><ul>
<li><code>env.reset()</code>: Initialize environment</li>
<li><code>env.step(action)</code>: Apply action and receive next state, reward, done flag</li>
<li><code>env.render()</code>: Visualize environment</li>
<li><code>observation_space</code>, <code>action_space</code>: Define state&#x2F;action formats</li>
</ul>
<h1 id="RL-Tools-Components"><a href="#RL-Tools-Components" class="headerlink" title="RL Tools &amp; Components"></a>RL Tools &amp; Components</h1><p>Building effective reinforcement learning pipelines requires more than just algorithms â€” tooling is essential for stability, performance, and reproducibility. Here are some of the most important tools and components commonly used in practice:</p>
<p><strong>Optuna</strong> is a powerful hyperparameter optimization library that automates tuning for better performance. Itâ€™s considered essential for finding optimal learning rates, network architectures, and other key parameters.</p>
<p><strong>EvalCallback</strong> provides automated evaluation during training and saves the best-performing model. This helps avoid overfitting and ensures only the best version is deployed â€” an essential part of any RL workflow.</p>
<p><strong>VecNormalize</strong> normalizes observations and rewards, which can significantly stabilize training, especially in environments with varying scales. Itâ€™s a crucial component for consistent performance.</p>
<p><strong>CheckpointCallback</strong> periodically saves model checkpoints. This is highly recommended to prevent loss of progress and enable resuming long training sessions.</p>
<p><strong>CustomCallback</strong> allows developers to inject custom logging, metric tracking, or interventions during training. Itâ€™s recommended when standard callbacks donâ€™t meet specific needs.</p>
<p><strong>SubprocVecEnv</strong> enables parallel environment execution, speeding up data collection by running multiple environment instances simultaneously. Itâ€™s particularly beneficial when using on-policy algorithms like PPO that require a lot of experience per update.</p>
<p><strong>TensorBoard</strong> is a visualization toolkit that makes it easy to monitor metrics such as reward, loss, and learning rate over time. Itâ€™s widely recommended for debugging and presenting training progress.</p>
<p><strong>saving and loading normalization statistics</strong> (like those from <code>VecNormalize</code>) is a must-have for consistency between training and inference. Without restoring these stats, the agent may perform poorly during evaluation or deployment due to mismatched input scales.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>About Machine Learning ( Part 10: Reinforcement Learning )</p><p><a href="https://kongchenglc.github.io/blog/2025/03/11/Machine-Learning-10/">https://kongchenglc.github.io/blog/2025/03/11/Machine-Learning-10/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Cheng (Mike)</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2025-03-11</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2025-07-25</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/Machine-Learning/">Machine Learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/Reinforcement-Learning/">Reinforcement Learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/GYM/">GYM</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/Actor-Critic/">Actor-Critic</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/PPO/">PPO</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/2025/05/31/algorithm-methodology/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Algorithms</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2025/03/11/Transformer-3/"><span class="level-item">Transformer ( Part 3: Transformer Architecture )</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><script src="https://utteranc.es/client.js" repo="kongchenglc/blog" issue-term="pathname" label="comment" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/blog/img/profile-pic.jpg" alt="Cheng (Mike)"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Cheng (Mike)</p><p class="is-size-6 is-block">Software Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Canada</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/blog/archives"><p class="title">24</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/blog/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">40</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/kongchenglc" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Linkedin" href="https://www.linkedin.com/in/kongchenglc/"><i class="fab fa-linkedin"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">1</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#What-is-Reinforcement-Learning"><span class="level-left"><span class="level-item">2</span><span class="level-item">What is Reinforcement Learning?</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Key-Concepts"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Key Concepts</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Exploration-vs-Exploitation"><span class="level-left"><span class="level-item">3</span><span class="level-item">Exploration vs. Exploitation</span></span></a></li><li><a class="level is-mobile" href="#Markov-Decision-Process-MDP"><span class="level-left"><span class="level-item">4</span><span class="level-item">Markov Decision Process (MDP)</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Markov-Property"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">Markov Property</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Value-Function-and-Q-Function"><span class="level-left"><span class="level-item">5</span><span class="level-item">Value Function and Q-Function</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#State-Value-Function-V-s"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">State Value Function $V(s)$</span></span></a></li><li><a class="level is-mobile" href="#Action-Value-Function-Q-s-a"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">Action-Value Function $Q(s, a)$</span></span></a></li><li><a class="level is-mobile" href="#Bellman-Equation"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">Bellman Equation</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#For-the-value-function"><span class="level-left"><span class="level-item">5.3.1</span><span class="level-item">For the value function:</span></span></a></li><li><a class="level is-mobile" href="#For-the-Q-function"><span class="level-left"><span class="level-item">5.3.2</span><span class="level-item">For the Q-function:</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Actor-Critic-Architecture"><span class="level-left"><span class="level-item">6</span><span class="level-item">Actor-Critic Architecture</span></span></a></li><li><a class="level is-mobile" href="#PPO-Proximal-Policy-Optimization"><span class="level-left"><span class="level-item">7</span><span class="level-item">PPO (Proximal Policy Optimization)</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Workflow"><span class="level-left"><span class="level-item">7.1</span><span class="level-item">Workflow:</span></span></a></li><li><a class="level is-mobile" href="#PPO-Objectives"><span class="level-left"><span class="level-item">7.2</span><span class="level-item">PPO Objectives</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-Clipped-Policy-Objective"><span class="level-left"><span class="level-item">7.2.1</span><span class="level-item">1. Clipped Policy Objective:</span></span></a></li><li><a class="level is-mobile" href="#2-Value-Function-Loss"><span class="level-left"><span class="level-item">7.2.2</span><span class="level-item">2. Value Function Loss:</span></span></a></li><li><a class="level is-mobile" href="#3-Total-Objective-with-Entropy-Regularization"><span class="level-left"><span class="level-item">7.2.3</span><span class="level-item">3. Total Objective with Entropy Regularization:</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Why-Use-Advantage-Instead-of-Raw-Reward"><span class="level-left"><span class="level-item">7.3</span><span class="level-item">Why Use Advantage Instead of Raw Reward?</span></span></a></li><li><a class="level is-mobile" href="#What-is-KL-Divergence"><span class="level-left"><span class="level-item">7.4</span><span class="level-item">What is KL Divergence?</span></span></a></li></ul></li><li><a class="level is-mobile" href="#GYM-Gymnasium"><span class="level-left"><span class="level-item">8</span><span class="level-item">GYM / Gymnasium</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Key-Features"><span class="level-left"><span class="level-item">8.1</span><span class="level-item">Key Features:</span></span></a></li></ul></li><li><a class="level is-mobile" href="#RL-Tools-Components"><span class="level-left"><span class="level-item">9</span><span class="level-item">RL Tools &amp; Components</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/blog/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/">Mike&#039;s Blog</a><p class="is-size-7"><span>&copy; 2025 Cheng (Mike)</span>Â Â Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>Â &amp;Â <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">Â© 2024</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/blog/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><!--!--><script data-pjax src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">Ã—</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><!-- hexo injector body_end start -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    var tocElement = document.querySelector('#toc');
    if (tocElement) {
      tocElement.classList.add('is-sticky');
    }
  });
</script>
  <!-- hexo injector body_end end --></body></html>