<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>About Machine Learning ( Part 10: Reinforcement Learning ) - Cheng&#039;s Blog</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Cheng&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Cheng&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Reinforcement Learning (RL) is an exciting field of machine learning where an agent learns how to behave in an environment in order to maximize cumulative rewards. The foundation of RL lies in Markov"><meta property="og:type" content="blog"><meta property="og:title" content="About Machine Learning ( Part 10: Reinforcement Learning )"><meta property="og:url" content="https://kongchenglc.github.io/blog/2025/03/11/Machine-Learning-10/"><meta property="og:site_name" content="Cheng&#039;s Blog"><meta property="og:description" content="Reinforcement Learning (RL) is an exciting field of machine learning where an agent learns how to behave in an environment in order to maximize cumulative rewards. The foundation of RL lies in Markov"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://kongchenglc.github.io/blog/img/ML10-1.png"><meta property="og:image" content="https://kongchenglc.github.io/blog/img/ML10-2.png"><meta property="article:published_time" content="2025-03-11T21:21:59.000Z"><meta property="article:modified_time" content="2025-03-12T03:15:07.182Z"><meta property="article:author" content="Cheng"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Reinforcement Learning"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://kongchenglc.github.io/blog/img/ML10-1.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kongchenglc.github.io/blog/2025/03/11/Machine-Learning-10/"},"headline":"About Machine Learning ( Part 10: Reinforcement Learning )","image":["https://kongchenglc.github.io/blog/img/ML10-1.png","https://kongchenglc.github.io/blog/img/ML10-2.png"],"datePublished":"2025-03-11T21:21:59.000Z","dateModified":"2025-03-12T03:15:07.182Z","author":{"@type":"Person","name":"Cheng"},"publisher":{"@type":"Organization","name":"Cheng's Blog","logo":{"@type":"ImageObject","url":{"text":"Cheng's Blog"}}},"description":"Reinforcement Learning (RL) is an exciting field of machine learning where an agent learns how to behave in an environment in order to maximize cumulative rewards. The foundation of RL lies in Markov"}</script><link rel="canonical" href="https://kongchenglc.github.io/blog/2025/03/11/Machine-Learning-10/"><link rel="icon" href="/blog/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/">Cheng&#039;s Blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">Home</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-03-11T21:21:59.000Z" title="3/11/2025, 9:21:59 PM">2025-03-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2025-03-12T03:15:07.182Z" title="3/12/2025, 3:15:07 AM">2025-03-12</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Learning-Notes/">Learning Notes</a></span></div></div><h1 class="title is-3 is-size-4-mobile">About Machine Learning ( Part 10: Reinforcement Learning )</h1><div class="content"><p>Reinforcement Learning (RL) is an exciting field of machine learning where an agent learns how to behave in an environment in order to maximize cumulative rewards. The foundation of RL lies in <strong>Markov Decision Processes (MDPs)</strong>, which provide a mathematical framework for modeling decision-making scenarios. In this blog, we will explore the core concepts of RL and MDPs, and how they interconnect to help an agent learn optimal behavior.</p>
<span id="more"></span>

<h1 id="What-is-Reinforcement-Learning"><a href="#What-is-Reinforcement-Learning" class="headerlink" title="What is Reinforcement Learning?"></a>What is Reinforcement Learning?</h1><p>Reinforcement Learning is a paradigm where an agent interacts with an environment and learns how to take actions that maximize a certain notion of cumulative reward. The agent doesn’t know the optimal actions at the beginning and instead learns through trial and error. Here are the core components of RL:</p>
<ul>
<li><strong>Agent</strong>: The learner or decision maker.</li>
<li><strong>Environment</strong>: Everything the agent interacts with.</li>
<li><strong>State ($s$)</strong>: A description of the current situation in the environment.</li>
<li><strong>Action ($a$)</strong>: A choice made by the agent.</li>
<li><strong>Reward ($r$)</strong>: A feedback signal from the environment after an action is taken.</li>
<li><strong>Policy ($\pi$)</strong>: A strategy that defines the agent’s behavior, mapping states to actions.</li>
<li><strong>Value Function ($V(s)$)</strong>: A function that estimates how good it is for the agent to be in state $s$.</li>
</ul>
<p>In RL, the agent’s objective is to learn a policy that maximizes the total cumulative reward over time.</p>
<p><img src="/blog/./img/ML10-1.png" alt="https://research.google/blog/off-policy-estimation-for-infinite-horizon-reinforcement-learning/"></p>
<h2 id="Exploration-vs-Exploitation"><a href="#Exploration-vs-Exploitation" class="headerlink" title="Exploration vs. Exploitation"></a>Exploration vs. Exploitation</h2><p>One of the key challenges in RL is balancing <strong>exploration</strong> and <strong>exploitation</strong>. Exploration means trying out new actions to discover their outcomes, while exploitation means using the known best actions to maximize reward. Striking the right balance is crucial for learning an optimal policy.</p>
<h1 id="Markov-Decision-Process-MDP"><a href="#Markov-Decision-Process-MDP" class="headerlink" title="Markov Decision Process (MDP)"></a>Markov Decision Process (MDP)</h1><p>The formal model that defines an RL problem is the <strong>Markov Decision Process (MDP)</strong>. An MDP provides a mathematical framework for modeling decision-making in situations where outcomes are partially random and partially under the control of the agent. It is defined by the following components:</p>
<ol>
<li><strong>States ($S$)</strong>: A set of all possible states in the environment. Each state $s \in S$ represents the environment’s configuration at a specific time.</li>
<li><strong>Actions ($A$)</strong>: A set of all possible actions the agent can take. Actions $a \in A$ influence the state of the environment.</li>
<li><strong>State Transition Probability ($P(s’|s, a)$)</strong>: The probability of transitioning from state $s$ to state $s’$ by taking action $a$.</li>
<li><strong>Reward Function ($R(s, a)$)</strong>: The reward the agent receives after taking action $a$ in state $s$.</li>
<li><strong>Discount Factor ($\gamma$)</strong>: A value between 0 and 1 that represents the importance of future rewards relative to immediate rewards.</li>
</ol>
<p>An agent’s goal is to learn a policy $\pi(s)$ that maps states to actions in such a way that it maximizes the total expected reward.</p>
<h2 id="The-Markov-Property"><a href="#The-Markov-Property" class="headerlink" title="The Markov Property"></a>The Markov Property</h2><p>One key feature of MDPs is the <strong>Markov Property</strong>, which means that the future state depends only on the current state and action, and not on the history of past states. In other words, the process has <strong>no memory</strong>, and the future is independent of the past given the present state.</p>
<p>This property simplifies the decision-making process for the agent because it only needs to consider the current state when making decisions.</p>
<h1 id="Value-Function-and-Q-Function"><a href="#Value-Function-and-Q-Function" class="headerlink" title="Value Function and Q-Function"></a>Value Function and Q-Function</h1><p>To find an optimal policy, we use the <strong>value function</strong> and <strong>Q-function</strong>, which estimate the long-term reward the agent can expect.</p>
<h2 id="Value-Function"><a href="#Value-Function" class="headerlink" title="Value Function"></a>Value Function</h2><p>The <strong>value function</strong> $V(s)$ estimates the expected return (reward) the agent can achieve from a given state $s$. Mathematically, it is defined as:</p>
<p>$$<br>V(s) &#x3D; \mathbb{E} \left[ \sum_{t&#x3D;0}^{\infty} \gamma^t r_t \Big| s_0 &#x3D; s \right]<br>$$</p>
<p>Here, $r_t$ represents the reward at time step $t$, and $\gamma$ is the discount factor that determines the importance of future rewards.</p>
<h2 id="Q-Function"><a href="#Q-Function" class="headerlink" title="Q-Function"></a>Q-Function</h2><p>The <strong>Q-function</strong> $Q(s, a)$, or action-value function, estimates the expected return from a state-action pair. It is defined as:</p>
<p>$$<br>Q(s, a) &#x3D; \mathbb{E} \left[ \sum_{t&#x3D;0}^{\infty} \gamma^t r_t \Big| s_0 &#x3D; s, a_0 &#x3D; a \right]<br>$$</p>
<p>In this case, the agent evaluates the potential reward of taking action $a$ in state $s$ and following the optimal policy thereafter.</p>
<p><img src="/blog/./img/ML10-2.png" alt="https://bruceoutdoors.wordpress.com/tag/reinforcement-learning/"></p>
<h1 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h1><p>The <strong>Bellman equation</strong> provides a recursive relationship for the value function and Q-function. It expresses the value of a state in terms of the expected reward and the value of future states.</p>
<p>For the value function, the Bellman equation is:</p>
<p>$$<br>V(s) &#x3D; \max_a \left( R(s, a) + \gamma \sum_{s’} P(s’|s, a) V(s’) \right)<br>$$</p>
<p>For the Q-function, the Bellman equation is:</p>
<p>$$<br>Q(s, a) &#x3D; R(s, a) + \gamma \sum_{s’} P(s’|s, a) \max_{a’} Q(s’, a’)<br>$$</p>
<p>These recursive equations are fundamental for solving RL problems and finding the optimal policy.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>About Machine Learning ( Part 10: Reinforcement Learning )</p><p><a href="https://kongchenglc.github.io/blog/2025/03/11/Machine-Learning-10/">https://kongchenglc.github.io/blog/2025/03/11/Machine-Learning-10/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Cheng</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2025-03-11</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2025-03-12</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/Machine-Learning/">Machine Learning</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/Reinforcement-Learning/">Reinforcement Learning</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/2025/03/11/Transformer-3/"><span class="level-item">Transformer ( Part 3: Transformer Architecture )</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><script src="https://utteranc.es/client.js" repo="kongchenglc/blog" issue-term="pathname" label="comment" theme="github-light" crossorigin="anonymous" async></script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/blog/img/profile-pic.jpg" alt="Cheng"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Cheng</p><p class="is-size-6 is-block">Software Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Canada</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/blog/archives"><p class="title">22</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/blog/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">32</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/kongchenglc" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Defunct Chinese Blog" href="https://kongchenglc.github.io/blog/"><i class="fa-solid fa-link"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#What-is-Reinforcement-Learning"><span class="level-left"><span class="level-item">1</span><span class="level-item">What is Reinforcement Learning?</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Exploration-vs-Exploitation"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">Exploration vs. Exploitation</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Markov-Decision-Process-MDP"><span class="level-left"><span class="level-item">2</span><span class="level-item">Markov Decision Process (MDP)</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#The-Markov-Property"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">The Markov Property</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Value-Function-and-Q-Function"><span class="level-left"><span class="level-item">3</span><span class="level-item">Value Function and Q-Function</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Value-Function"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">Value Function</span></span></a></li><li><a class="level is-mobile" href="#Q-Function"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">Q-Function</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Bellman-Equation"><span class="level-left"><span class="level-item">4</span><span class="level-item">Bellman Equation</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/blog/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/">Cheng&#039;s Blog</a><p class="is-size-7"><span>&copy; 2025 Cheng</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2024</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/kongchenglc"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/blog/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/blog/js/pjax.js"></script><!--!--><script data-pjax src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><!-- hexo injector body_end start --><script src="/blog/js/custom-css.js"></script><!-- hexo injector body_end end --></body></html>